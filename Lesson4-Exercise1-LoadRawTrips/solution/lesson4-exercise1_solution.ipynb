{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4 - Exercise 1: Load Staging Table with COPY Command\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this exercise, you will:\n",
    "\n",
    "1. Create your own S3 bucket for staging data\n",
    "2. Upload data files to S3\n",
    "3. Use the COPY command to load data into Redshift\n",
    "4. Understand COPY options for different file formats\n",
    "5. Validate successful data loads\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS credentials configured in `aws_config.py`\n",
    "- `van_transit_trips_postgres.csv` data file\n",
    "- Redshift Serverless workgroup available\n",
    "\n",
    "## Context\n",
    "\n",
    "In production Redshift workflows, data flows from ETL scripts to S3, then into Redshift via the COPY command. The COPY command reads files in parallel across all cluster nodes, making it far more efficient than row-by-row INSERTs.\n",
    "\n",
    "**Note on Authentication**: In production, you would use `IAM_ROLE` for secure access. In this workspace, we use temporary credentials with the `CREDENTIALS` parameter since no default IAM role is configured on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n",
      "   - pandas version: 2.3.1\n",
      "   - numpy version: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "# ========= Imports\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"   - pandas version: {pd.__version__}\")\n",
    "print(f\"   - numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n",
      "   - AWS Region: us-east-1\n",
      "   - Redshift Database: dev\n",
      "   - Redshift Workgroup: udacity-dwh-wg\n",
      "   - Credentials: Loaded\n"
     ]
    }
   ],
   "source": [
    "# ========= Load AWS Credentials\n",
    "import aws_config\n",
    "\n",
    "AWS_REGION = os.getenv('AWS_REGION')\n",
    "REDSHIFT_DATABASE = os.getenv('REDSHIFT_DATABASE')\n",
    "REDSHIFT_WORKGROUP = os.getenv('REDSHIFT_WORKGROUP')\n",
    "\n",
    "# Get credentials for COPY command\n",
    "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "AWS_SESSION_TOKEN = os.getenv('AWS_SESSION_TOKEN')\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"   - AWS Region: {AWS_REGION}\")\n",
    "print(f\"   - Redshift Database: {REDSHIFT_DATABASE}\")\n",
    "print(f\"   - Redshift Workgroup: {REDSHIFT_WORKGROUP}\")\n",
    "print(f\"   - Credentials: {'Loaded' if AWS_ACCESS_KEY_ID else 'Missing'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ========= Helper Function: Execute Redshift Query\n",
    "def execute_redshift_query(sql, fetch_results=True):\n",
    "    \"\"\"Execute a SQL query on Redshift Serverless and optionally return results.\"\"\"\n",
    "    client = boto3.client('redshift-data', region_name=AWS_REGION)\n",
    "    \n",
    "    # Execute query\n",
    "    response = client.execute_statement(\n",
    "        WorkgroupName=REDSHIFT_WORKGROUP,\n",
    "        Database=REDSHIFT_DATABASE,\n",
    "        Sql=sql\n",
    "    )\n",
    "    query_id = response['Id']\n",
    "    \n",
    "    # Wait for completion\n",
    "    status = 'SUBMITTED'\n",
    "    while status in ['SUBMITTED', 'PICKED', 'STARTED']:\n",
    "        time.sleep(1)\n",
    "        status_response = client.describe_statement(Id=query_id)\n",
    "        status = status_response['Status']\n",
    "    \n",
    "    if status == 'FAILED':\n",
    "        error = status_response.get('Error', 'Unknown error')\n",
    "        raise Exception(f\"Query failed: {error}\")\n",
    "    \n",
    "    # Fetch results if requested\n",
    "    if fetch_results and status == 'FINISHED':\n",
    "        try:\n",
    "            result = client.get_statement_result(Id=query_id)\n",
    "            columns = [col['name'] for col in result['ColumnMetadata']]\n",
    "            rows = []\n",
    "            for record in result['Records']:\n",
    "                row = [list(field.values())[0] if field else None for field in record]\n",
    "                rows.append(row)\n",
    "            return pd.DataFrame(rows, columns=columns)\n",
    "        except client.exceptions.ResourceNotFoundException:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Create S3 Bucket for Staging\n",
    "\n",
    "We'll create a unique S3 bucket to store our staging data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Created bucket 'udacity-redshift-staging-4a247f6b'\n",
      "\n",
      "S3 URI: s3://udacity-redshift-staging-4a247f6b/\n"
     ]
    }
   ],
   "source": [
    "# ========= Create S3 bucket\n",
    "s3 = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "# Generate unique bucket name\n",
    "unique_id = str(uuid.uuid4())[:8]\n",
    "BUCKET_NAME = f\"udacity-redshift-staging-{unique_id}\"\n",
    "\n",
    "try:\n",
    "    if AWS_REGION == 'us-east-1':\n",
    "        s3.create_bucket(Bucket=BUCKET_NAME)\n",
    "    else:\n",
    "        s3.create_bucket(\n",
    "            Bucket=BUCKET_NAME,\n",
    "            CreateBucketConfiguration={'LocationConstraint': AWS_REGION}\n",
    "        )\n",
    "    print(f\"SUCCESS: Created bucket '{BUCKET_NAME}'\")\n",
    "except ClientError as e:\n",
    "    if 'BucketAlreadyOwnedByYou' in str(e):\n",
    "        print(f\"Bucket already exists: {BUCKET_NAME}\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "print(f\"\\nS3 URI: s3://{BUCKET_NAME}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Review and Upload Transit Trip Data to S3\n",
    "\n",
    "We'll use the Vancouver Transit trips dataset which contains real transit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: data/van_transit_trips_postgres.csv\n",
      "Shape: 2,500 rows x 23 columns\n",
      "\n",
      "Columns (23):\n",
      "    1. trip_id\n",
      "    2. rider_id\n",
      "    3. route_id\n",
      "    4. mode\n",
      "    5. origin_station_id\n",
      "    6. destination_station_id\n",
      "    7. board_datetime\n",
      "    8. alight_datetime\n",
      "    9. country\n",
      "   10. province\n",
      "   11. fare_class\n",
      "   12. payment_method\n",
      "   13. transfers\n",
      "   14. zones_charged\n",
      "   15. distance_km\n",
      "   16. base_fare_cad\n",
      "   17. discount_rate\n",
      "   18. discount_amount_cad\n",
      "   19. yvr_addfare_cad\n",
      "   20. total_fare_cad\n",
      "   21. on_time_arrival\n",
      "   22. service_disruption\n",
      "   23. polyline_stations\n"
     ]
    }
   ],
   "source": [
    "# ========= Review the source data\n",
    "csv_path = \"data/van_transit_trips_postgres.csv\"\n",
    "\n",
    "trips_df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Dataset: {csv_path}\")\n",
    "print(f\"Shape: {trips_df.shape[0]:,} rows x {trips_df.shape[1]} columns\")\n",
    "print(f\"\\nColumns ({len(trips_df.columns)}):\")\n",
    "for i, col in enumerate(trips_df.columns, 1):\n",
    "    print(f\"   {i:2}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>rider_id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>mode</th>\n",
       "      <th>origin_station_id</th>\n",
       "      <th>destination_station_id</th>\n",
       "      <th>board_datetime</th>\n",
       "      <th>alight_datetime</th>\n",
       "      <th>country</th>\n",
       "      <th>province</th>\n",
       "      <th>...</th>\n",
       "      <th>zones_charged</th>\n",
       "      <th>distance_km</th>\n",
       "      <th>base_fare_cad</th>\n",
       "      <th>discount_rate</th>\n",
       "      <th>discount_amount_cad</th>\n",
       "      <th>yvr_addfare_cad</th>\n",
       "      <th>total_fare_cad</th>\n",
       "      <th>on_time_arrival</th>\n",
       "      <th>service_disruption</th>\n",
       "      <th>polyline_stations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T100000</td>\n",
       "      <td>R33247</td>\n",
       "      <td>R111</td>\n",
       "      <td>bus</td>\n",
       "      <td>S021</td>\n",
       "      <td>S004</td>\n",
       "      <td>2024-01-31 10:45:08</td>\n",
       "      <td>2024-01-31 11:12:09</td>\n",
       "      <td>CA</td>\n",
       "      <td>BC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.26</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.32</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>S001|S025|S009|S008|S009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T100001</td>\n",
       "      <td>R43159</td>\n",
       "      <td>R033</td>\n",
       "      <td>bus</td>\n",
       "      <td>S005</td>\n",
       "      <td>S025</td>\n",
       "      <td>2024-08-08 00:16:41</td>\n",
       "      <td>2024-08-08 00:44:35</td>\n",
       "      <td>CA</td>\n",
       "      <td>BC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>11.66</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.17</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>S004|S023|S025|S030|S018|S003|S020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T100002</td>\n",
       "      <td>R18110</td>\n",
       "      <td>R001</td>\n",
       "      <td>bus</td>\n",
       "      <td>S014</td>\n",
       "      <td>S002</td>\n",
       "      <td>2024-05-28 02:42:12</td>\n",
       "      <td>2024-05-28 03:14:48</td>\n",
       "      <td>CA</td>\n",
       "      <td>BC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>15.35</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>S001|S004|S008|S009|S018|S021|S001|S019|S007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T100003</td>\n",
       "      <td>R97939</td>\n",
       "      <td>R023</td>\n",
       "      <td>seabus</td>\n",
       "      <td>S023</td>\n",
       "      <td>S021</td>\n",
       "      <td>2025-06-14 06:40:38</td>\n",
       "      <td>2025-06-14 06:53:33</td>\n",
       "      <td>CA</td>\n",
       "      <td>BC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.74</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.12</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>S023|S018|S014|S008|S016|S020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T100004</td>\n",
       "      <td>R85766</td>\n",
       "      <td>R103</td>\n",
       "      <td>seabus</td>\n",
       "      <td>S009</td>\n",
       "      <td>S027</td>\n",
       "      <td>2024-01-29 18:06:53</td>\n",
       "      <td>2024-01-29 18:21:56</td>\n",
       "      <td>CA</td>\n",
       "      <td>BC</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7.99</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.51</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>S028|S001|S026|S027|S006|S024|S014|S011|S009|S005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_id rider_id route_id    mode origin_station_id destination_station_id  \\\n",
       "0  T100000   R33247     R111     bus              S021                   S004   \n",
       "1  T100001   R43159     R033     bus              S005                   S025   \n",
       "2  T100002   R18110     R001     bus              S014                   S002   \n",
       "3  T100003   R97939     R023  seabus              S023                   S021   \n",
       "4  T100004   R85766     R103  seabus              S009                   S027   \n",
       "\n",
       "        board_datetime      alight_datetime country province  ...  \\\n",
       "0  2024-01-31 10:45:08  2024-01-31 11:12:09      CA       BC  ...   \n",
       "1  2024-08-08 00:16:41  2024-08-08 00:44:35      CA       BC  ...   \n",
       "2  2024-05-28 02:42:12  2024-05-28 03:14:48      CA       BC  ...   \n",
       "3  2025-06-14 06:40:38  2025-06-14 06:53:33      CA       BC  ...   \n",
       "4  2024-01-29 18:06:53  2024-01-29 18:21:56      CA       BC  ...   \n",
       "\n",
       "  zones_charged distance_km  base_fare_cad  discount_rate  \\\n",
       "0             1        7.26           3.32           0.00   \n",
       "1             1       11.66           3.17           0.00   \n",
       "2             1       15.35           3.12           0.32   \n",
       "3             1        4.74           3.12           0.32   \n",
       "4             2        7.99           4.51           0.00   \n",
       "\n",
       "   discount_amount_cad  yvr_addfare_cad  total_fare_cad  on_time_arrival  \\\n",
       "0                  0.0              0.0            3.32             True   \n",
       "1                  0.0              0.0            3.17             True   \n",
       "2                  1.0              0.0            2.12            False   \n",
       "3                  1.0              0.0            2.12             True   \n",
       "4                  0.0              0.0            4.51             True   \n",
       "\n",
       "   service_disruption                                  polyline_stations  \n",
       "0               False                           S001|S025|S009|S008|S009  \n",
       "1               False                 S004|S023|S025|S030|S018|S003|S020  \n",
       "2               False       S001|S004|S008|S009|S018|S021|S001|S019|S007  \n",
       "3               False                      S023|S018|S014|S008|S016|S020  \n",
       "4               False  S028|S001|S026|S027|S006|S024|S014|S011|S009|S005  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========= Preview the data\n",
    "print(\"Sample data (first 5 rows):\")\n",
    "trips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: s3://udacity-redshift-staging-4a247f6b/staging/trips/van_transit_trips.csv\n",
      "   - Records: 2,500\n",
      "   - Size: 459,888 bytes\n"
     ]
    }
   ],
   "source": [
    "# ========= Upload CSV to S3\n",
    "s3_key = 'staging/trips/van_transit_trips.csv'\n",
    "\n",
    "# Upload the file\n",
    "with open(csv_path, 'rb') as f:\n",
    "    s3.put_object(\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key=s3_key,\n",
    "        Body=f\n",
    "    )\n",
    "\n",
    "# Get file size\n",
    "response = s3.head_object(Bucket=BUCKET_NAME, Key=s3_key)\n",
    "file_size = response['ContentLength']\n",
    "\n",
    "print(f\"Uploaded: s3://{BUCKET_NAME}/{s3_key}\")\n",
    "print(f\"   - Records: {len(trips_df):,}\")\n",
    "print(f\"   - Size: {file_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in staging/trips/:\n",
      "   - staging/trips/van_transit_trips.csv (459,888 bytes)\n"
     ]
    }
   ],
   "source": [
    "# ========= Verify the upload\n",
    "response = s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix='staging/trips/')\n",
    "\n",
    "print(\"Files in staging/trips/:\")\n",
    "for obj in response.get('Contents', []):\n",
    "    print(f\"   - {obj['Key']} ({obj['Size']:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Create Staging Table in Redshift\n",
    "\n",
    "Before we can COPY data, we need a target table in Redshift. The table schema must match the CSV columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Created table public.stg_trips_raw\n",
      "   - Columns: 23 (matching CSV schema)\n"
     ]
    }
   ],
   "source": [
    "# ========= Create staging table with correct schema\n",
    "create_table_sql = \"\"\"\n",
    "DROP TABLE IF EXISTS public.stg_trips_raw;\n",
    "\n",
    "CREATE TABLE public.stg_trips_raw (\n",
    "    trip_id               VARCHAR(32),\n",
    "    rider_id              VARCHAR(32),\n",
    "    route_id              VARCHAR(32),\n",
    "    mode                  VARCHAR(16),\n",
    "    origin_station_id     VARCHAR(32),\n",
    "    destination_station_id VARCHAR(32),\n",
    "    board_datetime        TIMESTAMP,\n",
    "    alight_datetime       TIMESTAMP,\n",
    "    country               VARCHAR(8),\n",
    "    province              VARCHAR(8),\n",
    "    fare_class            VARCHAR(16),\n",
    "    payment_method        VARCHAR(32),\n",
    "    transfers             INTEGER,\n",
    "    zones_charged         INTEGER,\n",
    "    distance_km           DECIMAL(10,2),\n",
    "    base_fare_cad         DECIMAL(10,2),\n",
    "    discount_rate         DECIMAL(5,3),\n",
    "    discount_amount_cad   DECIMAL(10,2),\n",
    "    yvr_addfare_cad       DECIMAL(10,2),\n",
    "    total_fare_cad        DECIMAL(10,2),\n",
    "    on_time_arrival       BOOLEAN,\n",
    "    service_disruption    BOOLEAN,\n",
    "    polyline_stations     VARCHAR(512)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "execute_redshift_query(create_table_sql, fetch_results=False)\n",
    "print(\"SUCCESS: Created table public.stg_trips_raw\")\n",
    "print(f\"   - Columns: 23 (matching CSV schema)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Load Data with COPY Command\n",
    "\n",
    "Now we'll use the COPY command to load data from S3 into Redshift.\n",
    "\n",
    "### COPY Command Components\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|----------|\n",
    "| `COPY table_name` | Target table for the load |\n",
    "| `FROM 's3://...'` | S3 path to data files |\n",
    "| `CREDENTIALS` | AWS credentials for S3 access (workspace) |\n",
    "| `IAM_ROLE` | IAM role ARN for S3 access (production) |\n",
    "| `FORMAT AS CSV` | File format |\n",
    "| `IGNOREHEADER 1` | Skip header row |\n",
    "\n",
    "### Production vs Workspace Authentication\n",
    "\n",
    "```sql\n",
    "-- PRODUCTION: Use IAM Role (recommended)\n",
    "COPY table FROM 's3://bucket/path/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789:role/RedshiftS3Role'\n",
    "FORMAT AS CSV;\n",
    "\n",
    "-- WORKSPACE: Use temporary credentials\n",
    "COPY table FROM 's3://bucket/path/'\n",
    "CREDENTIALS 'aws_access_key_id=...;aws_secret_access_key=...;token=...'\n",
    "FORMAT AS CSV;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing COPY command...\n",
      "============================================================\n",
      "\n",
      "COPY public.stg_trips_raw\n",
      "FROM 's3://udacity-redshift-staging-4a247f6b/staging/trips/'\n",
      "CREDENTIALS '***hidden***'\n",
      "FORMAT AS CSV\n",
      "IGNOREHEADER 1\n",
      "TIMEFORMAT 'auto'\n",
      "DATEFORMAT 'auto'\n",
      "REGION 'us-east-1';\n",
      "\n",
      "============================================================\n",
      "\n",
      "SUCCESS: COPY completed in 21.7 seconds\n"
     ]
    }
   ],
   "source": [
    "# ========= Execute COPY command\n",
    "s3_path = f\"s3://{BUCKET_NAME}/staging/trips/\"\n",
    "\n",
    "copy_sql = f\"\"\"\n",
    "COPY public.stg_trips_raw\n",
    "FROM '{s3_path}'\n",
    "CREDENTIALS 'aws_access_key_id={AWS_ACCESS_KEY_ID};aws_secret_access_key={AWS_SECRET_ACCESS_KEY};token={AWS_SESSION_TOKEN}'\n",
    "FORMAT AS CSV\n",
    "IGNOREHEADER 1\n",
    "TIMEFORMAT 'auto'\n",
    "DATEFORMAT 'auto'\n",
    "REGION '{AWS_REGION}';\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executing COPY command...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "COPY public.stg_trips_raw\n",
    "FROM '{s3_path}'\n",
    "CREDENTIALS '***hidden***'\n",
    "FORMAT AS CSV\n",
    "IGNOREHEADER 1\n",
    "TIMEFORMAT 'auto'\n",
    "DATEFORMAT 'auto'\n",
    "REGION '{AWS_REGION}';\n",
    "\"\"\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "execute_redshift_query(copy_sql, fetch_results=False)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nSUCCESS: COPY completed in {elapsed:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Validate the Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Count Validation:\n",
      "   - CSV file:  2,500 rows\n",
      "   - Database:  2,500 rows\n",
      "   - Match: YES\n"
     ]
    }
   ],
   "source": [
    "# ========= Verify row count\n",
    "result = execute_redshift_query(\"SELECT COUNT(*) as row_count FROM public.stg_trips_raw;\")\n",
    "db_count = int(result['row_count'].iloc[0])\n",
    "csv_count = len(trips_df)\n",
    "\n",
    "print(\"Row Count Validation:\")\n",
    "print(f\"   - CSV file:  {csv_count:,} rows\")\n",
    "print(f\"   - Database:  {db_count:,} rows\")\n",
    "print(f\"   - Match: {'YES' if db_count == csv_count else 'NO - Check STL_LOAD_ERRORS'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL Value Check:\n",
      "   - total_rows: 2500\n",
      "   - null_trip_id: 0\n",
      "   - null_rider_id: 0\n",
      "   - null_board_datetime: 0\n",
      "   - null_fare_class: 0\n",
      "   - null_total_fare: 0\n"
     ]
    }
   ],
   "source": [
    "# ========= Check for NULL values in key columns\n",
    "null_check_sql = \"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_rows,\n",
    "    SUM(CASE WHEN trip_id IS NULL THEN 1 ELSE 0 END) as null_trip_id,\n",
    "    SUM(CASE WHEN rider_id IS NULL THEN 1 ELSE 0 END) as null_rider_id,\n",
    "    SUM(CASE WHEN board_datetime IS NULL THEN 1 ELSE 0 END) as null_board_datetime,\n",
    "    SUM(CASE WHEN fare_class IS NULL THEN 1 ELSE 0 END) as null_fare_class,\n",
    "    SUM(CASE WHEN total_fare_cad IS NULL THEN 1 ELSE 0 END) as null_total_fare\n",
    "FROM public.stg_trips_raw;\n",
    "\"\"\"\n",
    "\n",
    "result = execute_redshift_query(null_check_sql)\n",
    "print(\"NULL Value Check:\")\n",
    "for col in result.columns:\n",
    "    print(f\"   - {col}: {result[col].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data from Staging Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>rider_id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>mode</th>\n",
       "      <th>fare_class</th>\n",
       "      <th>total_fare_cad</th>\n",
       "      <th>board_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T100000</td>\n",
       "      <td>R33247</td>\n",
       "      <td>R111</td>\n",
       "      <td>bus</td>\n",
       "      <td>adult</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2024-01-31 10:45:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T100001</td>\n",
       "      <td>R43159</td>\n",
       "      <td>R033</td>\n",
       "      <td>bus</td>\n",
       "      <td>adult</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2024-08-08 00:16:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T100002</td>\n",
       "      <td>R18110</td>\n",
       "      <td>R001</td>\n",
       "      <td>bus</td>\n",
       "      <td>youth</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2024-05-28 02:42:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T100003</td>\n",
       "      <td>R97939</td>\n",
       "      <td>R023</td>\n",
       "      <td>seabus</td>\n",
       "      <td>youth</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2025-06-14 06:40:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T100004</td>\n",
       "      <td>R85766</td>\n",
       "      <td>R103</td>\n",
       "      <td>seabus</td>\n",
       "      <td>adult</td>\n",
       "      <td>4.51</td>\n",
       "      <td>2024-01-29 18:06:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_id rider_id route_id    mode fare_class total_fare_cad  \\\n",
       "0  T100000   R33247     R111     bus      adult           3.32   \n",
       "1  T100001   R43159     R033     bus      adult           3.17   \n",
       "2  T100002   R18110     R001     bus      youth           2.12   \n",
       "3  T100003   R97939     R023  seabus      youth           2.12   \n",
       "4  T100004   R85766     R103  seabus      adult           4.51   \n",
       "\n",
       "        board_datetime  \n",
       "0  2024-01-31 10:45:08  \n",
       "1  2024-08-08 00:16:41  \n",
       "2  2024-05-28 02:42:12  \n",
       "3  2025-06-14 06:40:38  \n",
       "4  2024-01-29 18:06:53  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========= Sample data from staging table\n",
    "sample_sql = \"\"\"\n",
    "SELECT \n",
    "    trip_id, rider_id, route_id, mode, fare_class, \n",
    "    total_fare_cad, board_datetime\n",
    "FROM public.stg_trips_raw\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample Data from Staging Table:\")\n",
    "execute_redshift_query(sample_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging Table Summary:\n",
      "============================================================\n",
      "   total_trips: 2500\n",
      "   unique_riders: 2455\n",
      "   unique_routes: 120\n",
      "   fare_classes: 5\n",
      "   earliest_trip: 2024-01-01 01:08:58\n",
      "   latest_trip: 2025-06-29 20:37:07\n",
      "   total_revenue: 7570.32\n",
      "   avg_fare: 3.02\n"
     ]
    }
   ],
   "source": [
    "# ========= Summary statistics\n",
    "summary_sql = \"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_trips,\n",
    "    COUNT(DISTINCT rider_id) as unique_riders,\n",
    "    COUNT(DISTINCT route_id) as unique_routes,\n",
    "    COUNT(DISTINCT fare_class) as fare_classes,\n",
    "    MIN(board_datetime) as earliest_trip,\n",
    "    MAX(board_datetime) as latest_trip,\n",
    "    ROUND(SUM(total_fare_cad), 2) as total_revenue,\n",
    "    ROUND(AVG(total_fare_cad), 2) as avg_fare\n",
    "FROM public.stg_trips_raw;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Staging Table Summary:\")\n",
    "print(\"=\" * 60)\n",
    "result = execute_redshift_query(summary_sql)\n",
    "for col in result.columns:\n",
    "    print(f\"   {col}: {result[col].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: COPY Options Reference\n",
    "\n",
    "The COPY command supports many options for different data scenarios.\n",
    "\n",
    "### Common COPY Options\n",
    "\n",
    "| Option | Purpose | Example |\n",
    "|--------|---------|----------|\n",
    "| `IGNOREHEADER n` | Skip first n rows (header) | `IGNOREHEADER 1` |\n",
    "| `DELIMITER 'char'` | Column separator | `DELIMITER ','` |\n",
    "| `TIMEFORMAT` | Timestamp parsing | `TIMEFORMAT 'auto'` |\n",
    "| `DATEFORMAT` | Date parsing | `DATEFORMAT 'auto'` |\n",
    "| `BLANKSASNULL` | Empty strings become NULL | - |\n",
    "| `EMPTYASNULL` | Empty fields become NULL | - |\n",
    "| `MAXERROR n` | Allow up to n errors | `MAXERROR 100` |\n",
    "| `TRUNCATECOLUMNS` | Truncate data exceeding column width | - |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY Command Examples\n",
      "============================================================\n",
      "\n",
      "1. PRODUCTION: Basic CSV Load with IAM Role\n",
      "--------------------------------------------\n",
      "COPY public.stg_trips_raw\n",
      "FROM 's3://your-bucket/staging/trips/'\n",
      "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
      "FORMAT AS CSV\n",
      "IGNOREHEADER 1;\n",
      "\n",
      "\n",
      "2. Robust CSV Load with Error Handling\n",
      "--------------------------------------\n",
      "COPY public.stg_trips_raw\n",
      "FROM 's3://your-bucket/staging/trips/'\n",
      "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
      "FORMAT AS CSV\n",
      "IGNOREHEADER 1\n",
      "DELIMITER ','\n",
      "TIMEFORMAT 'auto'\n",
      "DATEFORMAT 'auto'\n",
      "BLANKSASNULL\n",
      "EMPTYASNULL\n",
      "ACCEPTINVCHARS AS '?'\n",
      "MAXERROR 100\n",
      "TRUNCATECOLUMNS;\n",
      "\n",
      "\n",
      "3. Parquet Format (Recommended for Production)\n",
      "----------------------------------------------\n",
      "COPY public.stg_trips_raw\n",
      "FROM 's3://your-bucket/staging/trips/'\n",
      "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
      "FORMAT AS PARQUET;\n",
      "\n",
      "Note: Parquet is 5-10x faster than CSV and doesn't need\n",
      "      IGNOREHEADER or DELIMITER options.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========= COPY Command Reference Examples\n",
    "\n",
    "print(\"COPY Command Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example 1: Basic CSV load (Production with IAM Role)\n",
    "print(\"\"\"\n",
    "1. PRODUCTION: Basic CSV Load with IAM Role\n",
    "--------------------------------------------\n",
    "COPY public.stg_trips_raw\n",
    "FROM 's3://your-bucket/staging/trips/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
    "FORMAT AS CSV\n",
    "IGNOREHEADER 1;\n",
    "\"\"\")\n",
    "\n",
    "# Example 2: CSV with error handling\n",
    "print(\"\"\"\n",
    "2. Robust CSV Load with Error Handling\n",
    "--------------------------------------\n",
    "COPY public.stg_trips_raw\n",
    "FROM 's3://your-bucket/staging/trips/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
    "FORMAT AS CSV\n",
    "IGNOREHEADER 1\n",
    "DELIMITER ','\n",
    "TIMEFORMAT 'auto'\n",
    "DATEFORMAT 'auto'\n",
    "BLANKSASNULL\n",
    "EMPTYASNULL\n",
    "ACCEPTINVCHARS AS '?'\n",
    "MAXERROR 100\n",
    "TRUNCATECOLUMNS;\n",
    "\"\"\")\n",
    "\n",
    "# Example 3: Parquet format (recommended for production)\n",
    "print(\"\"\"\n",
    "3. Parquet Format (Recommended for Production)\n",
    "----------------------------------------------\n",
    "COPY public.stg_trips_raw\n",
    "FROM 's3://your-bucket/staging/trips/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
    "FORMAT AS PARQUET;\n",
    "\n",
    "Note: Parquet is 5-10x faster than CSV and doesn't need\n",
    "      IGNOREHEADER or DELIMITER options.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Check for Load Errors\n",
    "\n",
    "If your COPY command fails, you can check the error logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Cannot access stl_load_errors (requires superuser privileges)\n",
      "\n",
      "In production environments, you would check this table to debug COPY failures.\n",
      "\n",
      "Alternative: If COPY fails, the error message in the exception usually\n",
      "contains enough information to diagnose the issue.\n"
     ]
    }
   ],
   "source": [
    "# ========= Check for COPY errors\n",
    "# Note: stl_load_errors requires superuser privileges\n",
    "# In this workspace, we may not have access to system tables\n",
    "\n",
    "error_sql = \"\"\"\n",
    "SELECT \n",
    "    starttime,\n",
    "    filename,\n",
    "    line_number,\n",
    "    colname,\n",
    "    err_reason\n",
    "FROM stl_load_errors\n",
    "ORDER BY starttime DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    errors = execute_redshift_query(error_sql)\n",
    "    if errors is not None and len(errors) > 0:\n",
    "        print(\"Recent COPY errors:\")\n",
    "        print(errors)\n",
    "    else:\n",
    "        print(\"No COPY errors found!\")\n",
    "except Exception as e:\n",
    "    if 'permission denied' in str(e).lower():\n",
    "        print(\"Note: Cannot access stl_load_errors (requires superuser privileges)\")\n",
    "        print(\"\\nIn production environments, you would check this table to debug COPY failures.\")\n",
    "        print(\"\\nAlternative: If COPY fails, the error message in the exception usually\")\n",
    "        print(\"contains enough information to diagnose the issue.\")\n",
    "    else:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup (Optional)\n",
    "\n",
    "Delete the S3 bucket when you're done to avoid storage charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= OPTIONAL: Delete S3 bucket\n",
    "# Uncomment the code below to delete the bucket after the exercise\n",
    "\n",
    "# print(\"Deleting S3 bucket and contents...\")\n",
    "# \n",
    "# # First delete all objects\n",
    "# response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "# for obj in response.get('Contents', []):\n",
    "#     s3.delete_object(Bucket=BUCKET_NAME, Key=obj['Key'])\n",
    "#     print(f\"   Deleted: {obj['Key']}\")\n",
    "# \n",
    "# # Then delete the bucket\n",
    "# s3.delete_bucket(Bucket=BUCKET_NAME)\n",
    "# print(f\"   Deleted bucket: {BUCKET_NAME}\")\n",
    "\n",
    "print(\"Cleanup skipped. Uncomment the code above to delete the S3 bucket.\")\n",
    "print(f\"\\nBucket to delete later: {BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **S3 Bucket Creation** - Created a unique bucket for staging data\n",
    "2. **Data Upload** - Uploaded CSV files to S3 using boto3\n",
    "3. **Table Creation** - Created a staging table matching the source schema\n",
    "4. **COPY Command** - Loaded data from S3 into Redshift using COPY\n",
    "5. **Validation** - Verified the data load with row counts and sample queries\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **COPY is faster** than INSERT for bulk loads (parallel across nodes)\n",
    "- **Schema must match** between source files and target table\n",
    "- **Check STL_LOAD_ERRORS** when loads fail or have fewer rows than expected\n",
    "- **Use IAM roles** in production instead of credentials strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production COPY Template\n",
    "\n",
    "```sql\n",
    "-- Idempotent staging load (PRODUCTION)\n",
    "TRUNCATE TABLE stg.trips_raw;\n",
    "\n",
    "COPY stg.trips_raw\n",
    "FROM 's3://your-bucket/staging/trips/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
    "FORMAT AS CSV\n",
    "IGNOREHEADER 1\n",
    "TIMEFORMAT 'auto';\n",
    "\n",
    "-- Validate\n",
    "SELECT COUNT(*) FROM stg.trips_raw;\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
