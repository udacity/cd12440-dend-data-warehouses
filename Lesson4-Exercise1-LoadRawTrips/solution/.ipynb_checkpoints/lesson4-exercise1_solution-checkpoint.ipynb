{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4 - Exercise 1: Load Staging Table with COPY Command\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this exercise, you will:\n",
    "\n",
    "1. Create your own S3 bucket for staging data\n",
    "2. Upload data files to S3\n",
    "3. Use the COPY command to load data into Redshift\n",
    "4. Understand COPY options for different file formats\n",
    "5. Validate successful data loads\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS credentials configured in `aws_config.py`\n",
    "- Redshift Serverless workgroup available\n",
    "\n",
    "## Context\n",
    "\n",
    "In production Redshift workflows, data flows from ETL scripts to S3, then into Redshift via the COPY command. The COPY command reads files in parallel across all cluster nodes, making it far more efficient than row-by-row INSERTs.\n",
    "\n",
    "**Note on Authentication**: In production, you would use `IAM_ROLE` for secure access. In this workspace, we use temporary credentials with the `CREDENTIALS` parameter since no default IAM role is configured on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n",
      "   - pandas version: 2.3.1\n",
      "   - numpy version: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "# ========= Imports\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"   - pandas version: {pd.__version__}\")\n",
    "print(f\"   - numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n",
      "   - AWS Region: us-east-1\n",
      "   - Redshift Database: dev\n",
      "   - Redshift Workgroup: udacity-dwh-wg\n",
      "   - Credentials: Loaded\n"
     ]
    }
   ],
   "source": [
    "# ========= Load AWS Credentials\n",
    "import aws_config\n",
    "\n",
    "AWS_REGION = os.getenv('AWS_REGION')\n",
    "REDSHIFT_DATABASE = os.getenv('REDSHIFT_DATABASE')\n",
    "REDSHIFT_WORKGROUP = os.getenv('REDSHIFT_WORKGROUP')\n",
    "\n",
    "# Get credentials for COPY command\n",
    "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "AWS_SESSION_TOKEN = os.getenv('AWS_SESSION_TOKEN')\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"   - AWS Region: {AWS_REGION}\")\n",
    "print(f\"   - Redshift Database: {REDSHIFT_DATABASE}\")\n",
    "print(f\"   - Redshift Workgroup: {REDSHIFT_WORKGROUP}\")\n",
    "print(f\"   - Credentials: {'Loaded' if AWS_ACCESS_KEY_ID else 'Missing'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ========= Helper Function: Execute Redshift Query\n",
    "def execute_redshift_query(sql, fetch_results=True):\n",
    "    \"\"\"Execute a SQL query on Redshift Serverless and optionally return results.\"\"\"\n",
    "    client = boto3.client('redshift-data', region_name=AWS_REGION)\n",
    "    \n",
    "    # Execute query\n",
    "    response = client.execute_statement(\n",
    "        WorkgroupName=REDSHIFT_WORKGROUP,\n",
    "        Database=REDSHIFT_DATABASE,\n",
    "        Sql=sql\n",
    "    )\n",
    "    query_id = response['Id']\n",
    "    \n",
    "    # Wait for completion\n",
    "    status = 'SUBMITTED'\n",
    "    while status in ['SUBMITTED', 'PICKED', 'STARTED']:\n",
    "        time.sleep(1)\n",
    "        status_response = client.describe_statement(Id=query_id)\n",
    "        status = status_response['Status']\n",
    "    \n",
    "    if status == 'FAILED':\n",
    "        error = status_response.get('Error', 'Unknown error')\n",
    "        raise Exception(f\"Query failed: {error}\")\n",
    "    \n",
    "    # Fetch results if requested\n",
    "    if fetch_results and status == 'FINISHED':\n",
    "        try:\n",
    "            result = client.get_statement_result(Id=query_id)\n",
    "            columns = [col['name'] for col in result['ColumnMetadata']]\n",
    "            rows = []\n",
    "            for record in result['Records']:\n",
    "                row = []\n",
    "                for field in record:\n",
    "                    value = list(field.values())[0] if field else None\n",
    "                    row.append(value)\n",
    "                rows.append(row)\n",
    "            return pd.DataFrame(rows, columns=columns)\n",
    "        except ClientError:\n",
    "            return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY credentials ready!\n"
     ]
    }
   ],
   "source": [
    "# ========= Build COPY credentials string\n",
    "# This creates the credentials parameter for the COPY command\n",
    "\n",
    "def get_copy_credentials():\n",
    "    \"\"\"Build the CREDENTIALS string for COPY command.\"\"\"\n",
    "    creds = f\"aws_access_key_id={AWS_ACCESS_KEY_ID};aws_secret_access_key={AWS_SECRET_ACCESS_KEY}\"\n",
    "    if AWS_SESSION_TOKEN:\n",
    "        creds += f\";token={AWS_SESSION_TOKEN}\"\n",
    "    return creds\n",
    "\n",
    "# Test that credentials are available\n",
    "if AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY:\n",
    "    print(\"COPY credentials ready!\")\n",
    "else:\n",
    "    print(\"WARNING: Missing AWS credentials. COPY command will fail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Create Your Own S3 Bucket\n",
    "\n",
    "Each student needs their own S3 bucket for staging data. We'll create one with a unique name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Created bucket 'udacity-redshift-staging-27070584'\n",
      "\n",
      "*** IMPORTANT: Save this bucket name ***\n",
      "    BUCKET_NAME = 'udacity-redshift-staging-27070584'\n"
     ]
    }
   ],
   "source": [
    "# ========= Create a unique S3 bucket for this workspace\n",
    "s3 = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "# Generate a unique bucket name\n",
    "BUCKET_NAME = f\"udacity-redshift-staging-{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "try:\n",
    "    # Create bucket (us-east-1 doesn't need LocationConstraint)\n",
    "    if AWS_REGION == 'us-east-1':\n",
    "        s3.create_bucket(Bucket=BUCKET_NAME)\n",
    "    else:\n",
    "        s3.create_bucket(\n",
    "            Bucket=BUCKET_NAME,\n",
    "            CreateBucketConfiguration={'LocationConstraint': AWS_REGION}\n",
    "        )\n",
    "    print(f\"SUCCESS: Created bucket '{BUCKET_NAME}'\")\n",
    "except ClientError as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "\n",
    "# Save bucket name for later use\n",
    "print(f\"\\n*** IMPORTANT: Save this bucket name ***\")\n",
    "print(f\"    BUCKET_NAME = '{BUCKET_NAME}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: s3://udacity-redshift-staging-27070584/staging/trips/\n",
      "Created: s3://udacity-redshift-staging-27070584/staging/stations/\n",
      "Created: s3://udacity-redshift-staging-27070584/staging/events/\n"
     ]
    }
   ],
   "source": [
    "# ========= Create folder structure in S3\n",
    "folders = [\n",
    "    'staging/trips/',\n",
    "    'staging/stations/',\n",
    "    'staging/events/'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    s3.put_object(Bucket=BUCKET_NAME, Key=folder)\n",
    "    print(f\"Created: s3://{BUCKET_NAME}/{folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Upload Sample Data to S3\n",
    "\n",
    "We'll create sample trip data and upload it to our S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1000 trip records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>rider_id</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>duration_minutes</th>\n",
       "      <th>distance_km</th>\n",
       "      <th>fare_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRIP_000001</td>\n",
       "      <td>RIDER_00103</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>2024-01-01 00:30:00</td>\n",
       "      <td>52</td>\n",
       "      <td>1.50</td>\n",
       "      <td>10.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRIP_000002</td>\n",
       "      <td>RIDER_00180</td>\n",
       "      <td>49</td>\n",
       "      <td>39</td>\n",
       "      <td>2024-01-01 00:15:00</td>\n",
       "      <td>2024-01-01 00:45:00</td>\n",
       "      <td>49</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRIP_000003</td>\n",
       "      <td>RIDER_00093</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>2024-01-01 00:30:00</td>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>40</td>\n",
       "      <td>14.09</td>\n",
       "      <td>9.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRIP_000004</td>\n",
       "      <td>RIDER_00015</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>2024-01-01 00:45:00</td>\n",
       "      <td>2024-01-01 01:15:00</td>\n",
       "      <td>34</td>\n",
       "      <td>12.42</td>\n",
       "      <td>24.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRIP_000005</td>\n",
       "      <td>RIDER_00107</td>\n",
       "      <td>42</td>\n",
       "      <td>41</td>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>2024-01-01 01:30:00</td>\n",
       "      <td>59</td>\n",
       "      <td>12.23</td>\n",
       "      <td>6.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       trip_id     rider_id  start_station_id  end_station_id  \\\n",
       "0  TRIP_000001  RIDER_00103                40               9   \n",
       "1  TRIP_000002  RIDER_00180                49              39   \n",
       "2  TRIP_000003  RIDER_00093                44              31   \n",
       "3  TRIP_000004  RIDER_00015                19              32   \n",
       "4  TRIP_000005  RIDER_00107                42              41   \n",
       "\n",
       "           start_time            end_time  duration_minutes  distance_km  \\\n",
       "0 2024-01-01 00:00:00 2024-01-01 00:30:00                52         1.50   \n",
       "1 2024-01-01 00:15:00 2024-01-01 00:45:00                49         0.58   \n",
       "2 2024-01-01 00:30:00 2024-01-01 01:00:00                40        14.09   \n",
       "3 2024-01-01 00:45:00 2024-01-01 01:15:00                34        12.42   \n",
       "4 2024-01-01 01:00:00 2024-01-01 01:30:00                59        12.23   \n",
       "\n",
       "   fare_usd  \n",
       "0     10.73  \n",
       "1     10.46  \n",
       "2      9.31  \n",
       "3     24.45  \n",
       "4      6.33  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========= Create sample trip data\n",
    "np.random.seed(42)\n",
    "n_records = 1000\n",
    "\n",
    "trips_df = pd.DataFrame({\n",
    "    'trip_id': [f'TRIP_{i:06d}' for i in range(1, n_records + 1)],\n",
    "    'rider_id': [f'RIDER_{np.random.randint(1, 201):05d}' for _ in range(n_records)],\n",
    "    'start_station_id': np.random.randint(1, 50, n_records),\n",
    "    'end_station_id': np.random.randint(1, 50, n_records),\n",
    "    'start_time': pd.date_range('2024-01-01', periods=n_records, freq='15min'),\n",
    "    'end_time': pd.date_range('2024-01-01 00:30:00', periods=n_records, freq='15min'),\n",
    "    'duration_minutes': np.random.randint(5, 60, n_records),\n",
    "    'distance_km': np.round(np.random.uniform(0.5, 15.0, n_records), 2),\n",
    "    'fare_usd': np.round(np.random.uniform(2.50, 25.00, n_records), 2)\n",
    "})\n",
    "\n",
    "print(f\"Created {len(trips_df)} trip records\")\n",
    "trips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: s3://udacity-redshift-staging-27070584/staging/trips/trips_2024_01.csv\n",
      "   - Records: 1000\n",
      "   - Size: 83,449 bytes\n"
     ]
    }
   ],
   "source": [
    "# ========= Upload CSV to S3\n",
    "csv_buffer = trips_df.to_csv(index=False)\n",
    "\n",
    "s3.put_object(\n",
    "    Bucket=BUCKET_NAME,\n",
    "    Key='staging/trips/trips_2024_01.csv',\n",
    "    Body=csv_buffer.encode('utf-8')\n",
    ")\n",
    "\n",
    "print(f\"Uploaded: s3://{BUCKET_NAME}/staging/trips/trips_2024_01.csv\")\n",
    "print(f\"   - Records: {len(trips_df)}\")\n",
    "print(f\"   - Size: {len(csv_buffer):,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in staging/trips/:\n",
      "   - staging/trips/ (0 bytes)\n",
      "   - staging/trips/trips_2024_01.csv (83,449 bytes)\n"
     ]
    }
   ],
   "source": [
    "# ========= Verify the upload\n",
    "response = s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix='staging/trips/')\n",
    "\n",
    "print(\"Files in staging/trips/:\")\n",
    "for obj in response.get('Contents', []):\n",
    "    print(f\"   - {obj['Key']} ({obj['Size']:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Create Staging Table in Redshift\n",
    "\n",
    "Before we can COPY data, we need a target table in Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Created table public.stg_trips_raw\n"
     ]
    }
   ],
   "source": [
    "# ========= Create staging table\n",
    "create_table_sql = \"\"\"\n",
    "DROP TABLE IF EXISTS public.stg_trips_raw;\n",
    "\n",
    "CREATE TABLE public.stg_trips_raw (\n",
    "    trip_id VARCHAR(50),\n",
    "    rider_id VARCHAR(50),\n",
    "    start_station_id INTEGER,\n",
    "    end_station_id INTEGER,\n",
    "    start_time TIMESTAMP,\n",
    "    end_time TIMESTAMP,\n",
    "    duration_minutes INTEGER,\n",
    "    distance_km DECIMAL(10,2),\n",
    "    fare_usd DECIMAL(10,2)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "execute_redshift_query(create_table_sql, fetch_results=False)\n",
    "print(\"SUCCESS: Created table public.stg_trips_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Load Data with COPY Command\n",
    "\n",
    "Now we'll use the COPY command to load data from S3 into Redshift.\n",
    "\n",
    "### COPY Command Components\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|----------|\n",
    "| `COPY table_name` | Target table for the load |\n",
    "| `FROM 's3://...'` | S3 path to data files |\n",
    "| `CREDENTIALS` | AWS credentials for S3 access (workspace) |\n",
    "| `IAM_ROLE` | IAM role ARN for S3 access (production) |\n",
    "| `FORMAT AS CSV` | File format |\n",
    "| `IGNOREHEADER 1` | Skip header row |\n",
    "\n",
    "### Production vs Workspace Authentication\n",
    "\n",
    "```sql\n",
    "-- PRODUCTION: Use IAM Role (recommended)\n",
    "COPY table FROM 's3://bucket/path/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789:role/RedshiftS3Role'\n",
    "FORMAT AS CSV;\n",
    "\n",
    "-- WORKSPACE: Use temporary credentials\n",
    "COPY table FROM 's3://bucket/path/'\n",
    "CREDENTIALS 'aws_access_key_id=...;aws_secret_access_key=...;token=...'\n",
    "FORMAT AS CSV;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing COPY command...\n",
      "============================================================\n",
      "\n",
      "COPY public.stg_trips_raw\n",
      "FROM 's3://udacity-redshift-staging-27070584/staging/trips/'\n",
      "CREDENTIALS '***hidden***'\n",
      "FORMAT AS CSV\n",
      "IGNOREHEADER 1\n",
      "TIMEFORMAT 'auto'\n",
      "DATEFORMAT 'auto'\n",
      "REGION 'us-east-1';\n",
      "\n",
      "============================================================\n",
      "\n",
      "SUCCESS: COPY command completed!\n"
     ]
    }
   ],
   "source": [
    "# ========= COPY data from S3 to Redshift\n",
    "copy_sql = f\"\"\"\n",
    "COPY public.stg_trips_raw\n",
    "FROM 's3://{BUCKET_NAME}/staging/trips/'\n",
    "CREDENTIALS '{get_copy_credentials()}'\n",
    "FORMAT AS CSV\n",
    "IGNOREHEADER 1\n",
    "TIMEFORMAT 'auto'\n",
    "DATEFORMAT 'auto'\n",
    "REGION '{AWS_REGION}';\n",
    "\"\"\"\n",
    "\n",
    "# Print a sanitized version (hide credentials)\n",
    "print(\"Executing COPY command...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "COPY public.stg_trips_raw\n",
    "FROM 's3://{BUCKET_NAME}/staging/trips/'\n",
    "CREDENTIALS '***hidden***'\n",
    "FORMAT AS CSV\n",
    "IGNOREHEADER 1\n",
    "TIMEFORMAT 'auto'\n",
    "DATEFORMAT 'auto'\n",
    "REGION '{AWS_REGION}';\n",
    "\"\"\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    execute_redshift_query(copy_sql, fetch_results=False)\n",
    "    print(\"\\nSUCCESS: COPY command completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: {e}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"  1. Check that your AWS credentials haven't expired\")\n",
    "    print(\"  2. Verify the S3 bucket and path exist\")\n",
    "    print(\"  3. Check STL_LOAD_ERRORS for detailed error info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Validation Results:\n",
      "   row_count        earliest_trip          latest_trip avg_fare\n",
      "0       1000  2024-01-01 00:00:00  2024-01-11 09:45:00    13.44\n"
     ]
    }
   ],
   "source": [
    "# ========= Validate the load\n",
    "validation_sql = \"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as row_count,\n",
    "    MIN(start_time) as earliest_trip,\n",
    "    MAX(start_time) as latest_trip,\n",
    "    ROUND(AVG(fare_usd), 2) as avg_fare\n",
    "FROM public.stg_trips_raw;\n",
    "\"\"\"\n",
    "\n",
    "result = execute_redshift_query(validation_sql)\n",
    "print(\"Load Validation Results:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>rider_id</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>duration_minutes</th>\n",
       "      <th>distance_km</th>\n",
       "      <th>fare_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRIP_000001</td>\n",
       "      <td>RIDER_00103</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>2024-01-01 00:30:00</td>\n",
       "      <td>52</td>\n",
       "      <td>1.50</td>\n",
       "      <td>10.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRIP_000002</td>\n",
       "      <td>RIDER_00180</td>\n",
       "      <td>49</td>\n",
       "      <td>39</td>\n",
       "      <td>2024-01-01 00:15:00</td>\n",
       "      <td>2024-01-01 00:45:00</td>\n",
       "      <td>49</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRIP_000003</td>\n",
       "      <td>RIDER_00093</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>2024-01-01 00:30:00</td>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>40</td>\n",
       "      <td>14.09</td>\n",
       "      <td>9.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRIP_000004</td>\n",
       "      <td>RIDER_00015</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>2024-01-01 00:45:00</td>\n",
       "      <td>2024-01-01 01:15:00</td>\n",
       "      <td>34</td>\n",
       "      <td>12.42</td>\n",
       "      <td>24.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRIP_000005</td>\n",
       "      <td>RIDER_00107</td>\n",
       "      <td>42</td>\n",
       "      <td>41</td>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>2024-01-01 01:30:00</td>\n",
       "      <td>59</td>\n",
       "      <td>12.23</td>\n",
       "      <td>6.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       trip_id     rider_id  start_station_id  end_station_id  \\\n",
       "0  TRIP_000001  RIDER_00103                40               9   \n",
       "1  TRIP_000002  RIDER_00180                49              39   \n",
       "2  TRIP_000003  RIDER_00093                44              31   \n",
       "3  TRIP_000004  RIDER_00015                19              32   \n",
       "4  TRIP_000005  RIDER_00107                42              41   \n",
       "\n",
       "            start_time             end_time  duration_minutes distance_km  \\\n",
       "0  2024-01-01 00:00:00  2024-01-01 00:30:00                52        1.50   \n",
       "1  2024-01-01 00:15:00  2024-01-01 00:45:00                49        0.58   \n",
       "2  2024-01-01 00:30:00  2024-01-01 01:00:00                40       14.09   \n",
       "3  2024-01-01 00:45:00  2024-01-01 01:15:00                34       12.42   \n",
       "4  2024-01-01 01:00:00  2024-01-01 01:30:00                59       12.23   \n",
       "\n",
       "  fare_usd  \n",
       "0    10.73  \n",
       "1    10.46  \n",
       "2     9.31  \n",
       "3    24.45  \n",
       "4     6.33  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========= View sample records\n",
    "sample_sql = \"\"\"\n",
    "SELECT * FROM public.stg_trips_raw LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "result = execute_redshift_query(sample_sql)\n",
    "print(\"Sample Records:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: COPY Options Reference\n",
    "\n",
    "The COPY command supports many options for different data scenarios.\n",
    "\n",
    "### Common COPY Options\n",
    "\n",
    "| Option | Purpose | Example |\n",
    "|--------|---------|----------|\n",
    "| `IGNOREHEADER n` | Skip first n rows (header) | `IGNOREHEADER 1` |\n",
    "| `DELIMITER 'char'` | Column separator | `DELIMITER ','` |\n",
    "| `TIMEFORMAT` | Timestamp parsing | `TIMEFORMAT 'auto'` |\n",
    "| `DATEFORMAT` | Date parsing | `DATEFORMAT 'auto'` |\n",
    "| `BLANKSASNULL` | Empty strings become NULL | - |\n",
    "| `EMPTYASNULL` | Empty fields become NULL | - |\n",
    "| `MAXERROR n` | Allow up to n errors | `MAXERROR 100` |\n",
    "| `TRUNCATECOLUMNS` | Truncate data exceeding column width | - |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY Command Examples\n",
      "============================================================\n",
      "\n",
      "1. PRODUCTION: Basic CSV Load with IAM Role\n",
      "--------------------------------------------\n",
      "COPY public.stg_trips_raw\n",
      "FROM 's3://your-bucket/staging/trips/'\n",
      "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
      "FORMAT AS CSV\n",
      "IGNOREHEADER 1;\n",
      "\n",
      "\n",
      "2. Robust CSV Load with Error Handling\n",
      "--------------------------------------\n",
      "COPY public.stg_trips_raw\n",
      "FROM 's3://your-bucket/staging/trips/'\n",
      "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
      "FORMAT AS CSV\n",
      "IGNOREHEADER 1\n",
      "DELIMITER ','\n",
      "TIMEFORMAT 'auto'\n",
      "DATEFORMAT 'auto'\n",
      "BLANKSASNULL\n",
      "EMPTYASNULL\n",
      "ACCEPTINVCHARS AS '?'\n",
      "MAXERROR 100\n",
      "TRUNCATECOLUMNS;\n",
      "\n",
      "\n",
      "3. Parquet Format (Recommended for Production)\n",
      "----------------------------------------------\n",
      "COPY public.stg_trips_raw\n",
      "FROM 's3://your-bucket/staging/trips/'\n",
      "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
      "FORMAT AS PARQUET;\n",
      "\n",
      "Note: Parquet is 5-10x faster than CSV and doesn't need\n",
      "      IGNOREHEADER or DELIMITER options.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========= COPY Command Reference Examples\n",
    "\n",
    "print(\"COPY Command Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example 1: Basic CSV load (Production with IAM Role)\n",
    "print(\"\"\"\n",
    "1. PRODUCTION: Basic CSV Load with IAM Role\n",
    "--------------------------------------------\n",
    "COPY public.stg_trips_raw\n",
    "FROM 's3://your-bucket/staging/trips/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
    "FORMAT AS CSV\n",
    "IGNOREHEADER 1;\n",
    "\"\"\")\n",
    "\n",
    "# Example 2: CSV with error handling\n",
    "print(\"\"\"\n",
    "2. Robust CSV Load with Error Handling\n",
    "--------------------------------------\n",
    "COPY public.stg_trips_raw\n",
    "FROM 's3://your-bucket/staging/trips/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
    "FORMAT AS CSV\n",
    "IGNOREHEADER 1\n",
    "DELIMITER ','\n",
    "TIMEFORMAT 'auto'\n",
    "DATEFORMAT 'auto'\n",
    "BLANKSASNULL\n",
    "EMPTYASNULL\n",
    "ACCEPTINVCHARS AS '?'\n",
    "MAXERROR 100\n",
    "TRUNCATECOLUMNS;\n",
    "\"\"\")\n",
    "\n",
    "# Example 3: Parquet format (recommended for production)\n",
    "print(\"\"\"\n",
    "3. Parquet Format (Recommended for Production)\n",
    "----------------------------------------------\n",
    "COPY public.stg_trips_raw\n",
    "FROM 's3://your-bucket/staging/trips/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
    "FORMAT AS PARQUET;\n",
    "\n",
    "Note: Parquet is 5-10x faster than CSV and doesn't need\n",
    "      IGNOREHEADER or DELIMITER options.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Check for Load Errors\n",
    "\n",
    "If your COPY command fails, you can check the error logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Cannot access stl_load_errors (requires superuser privileges)\n",
      "\n",
      "In production environments, you would check this table to debug COPY failures.\n",
      "\n",
      "Alternative: If COPY fails, the error message in the exception usually\n",
      "contains enough information to diagnose the issue.\n"
     ]
    }
   ],
   "source": [
    "# ========= Check for COPY errors\n",
    "# Note: stl_load_errors requires superuser privileges\n",
    "# In this workspace, we may not have access to system tables\n",
    "\n",
    "error_sql = \"\"\"\n",
    "SELECT \n",
    "    starttime,\n",
    "    filename,\n",
    "    line_number,\n",
    "    colname,\n",
    "    err_reason\n",
    "FROM stl_load_errors\n",
    "ORDER BY starttime DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    errors = execute_redshift_query(error_sql)\n",
    "    if errors is not None and len(errors) > 0:\n",
    "        print(\"Recent COPY errors:\")\n",
    "        print(errors)\n",
    "    else:\n",
    "        print(\"No COPY errors found!\")\n",
    "except Exception as e:\n",
    "    if 'permission denied' in str(e).lower():\n",
    "        print(\"Note: Cannot access stl_load_errors (requires superuser privileges)\")\n",
    "        print(\"\\nIn production environments, you would check this table to debug COPY failures.\")\n",
    "        print(\"\\nAlternative: If COPY fails, the error message in the exception usually\")\n",
    "        print(\"contains enough information to diagnose the issue.\")\n",
    "    else:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Idempotent Load Pattern\n",
    "\n",
    "In production, use TRUNCATE + COPY to make loads repeatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncating public.stg_trips_raw...\n",
      "Loading from s3://udacity-redshift-staging-27070584/staging/trips/...\n",
      "SUCCESS: Loaded 1,000 rows into public.stg_trips_raw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(1000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========= Idempotent load pattern\n",
    "def load_staging_table(table_name, s3_path, truncate_first=True):\n",
    "    \"\"\"Load a staging table from S3 with optional truncate.\"\"\"\n",
    "    \n",
    "    if truncate_first:\n",
    "        print(f\"Truncating {table_name}...\")\n",
    "        execute_redshift_query(f\"TRUNCATE TABLE {table_name};\", fetch_results=False)\n",
    "    \n",
    "    copy_sql = f\"\"\"\n",
    "    COPY {table_name}\n",
    "    FROM '{s3_path}'\n",
    "    CREDENTIALS '{get_copy_credentials()}'\n",
    "    FORMAT AS CSV\n",
    "    IGNOREHEADER 1\n",
    "    TIMEFORMAT 'auto'\n",
    "    REGION '{AWS_REGION}';\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading from {s3_path}...\")\n",
    "    execute_redshift_query(copy_sql, fetch_results=False)\n",
    "    \n",
    "    # Validate\n",
    "    count_result = execute_redshift_query(f\"SELECT COUNT(*) as cnt FROM {table_name};\")\n",
    "    row_count = count_result['cnt'].iloc[0]\n",
    "    print(f\"SUCCESS: Loaded {row_count:,} rows into {table_name}\")\n",
    "    \n",
    "    return row_count\n",
    "\n",
    "# Test the idempotent load\n",
    "load_staging_table(\n",
    "    'public.stg_trips_raw',\n",
    "    f's3://{BUCKET_NAME}/staging/trips/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Clean Up (Optional)\n",
    "\n",
    "Delete the S3 bucket when you're done. Note: The bucket will also be deleted when your Udacity session ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup code is commented out. Uncomment to delete your bucket.\n"
     ]
    }
   ],
   "source": [
    "# ========= Clean up S3 bucket (OPTIONAL - uncomment to run)\n",
    "# WARNING: This will delete your bucket and all its contents!\n",
    "\n",
    "# def delete_bucket(bucket_name):\n",
    "#     s3_resource = boto3.resource('s3', region_name=AWS_REGION)\n",
    "#     bucket = s3_resource.Bucket(bucket_name)\n",
    "#     \n",
    "#     # Delete all objects first\n",
    "#     bucket.objects.all().delete()\n",
    "#     print(f\"Deleted all objects in {bucket_name}\")\n",
    "#     \n",
    "#     # Delete the bucket\n",
    "#     bucket.delete()\n",
    "#     print(f\"Deleted bucket {bucket_name}\")\n",
    "\n",
    "# delete_bucket(BUCKET_NAME)\n",
    "\n",
    "print(\"Cleanup code is commented out. Uncomment to delete your bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Create your own S3 bucket** for staging data\n",
    "2. **COPY is the production standard** for loading data from S3 to Redshift\n",
    "3. **Use `IAM_ROLE` in production** for secure, managed access\n",
    "4. **Use `CREDENTIALS` in development** when IAM roles aren't available\n",
    "5. **Configure COPY options** based on your data format and quality\n",
    "6. **Make loads idempotent** with TRUNCATE + COPY pattern\n",
    "7. **Always validate** with row counts and sample queries\n",
    "\n",
    "### Production COPY Template\n",
    "\n",
    "```sql\n",
    "-- Idempotent staging load (PRODUCTION)\n",
    "TRUNCATE TABLE stg.trips_raw;\n",
    "\n",
    "COPY stg.trips_raw\n",
    "FROM 's3://your-bucket/staging/trips/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
    "FORMAT AS CSV\n",
    "IGNOREHEADER 1\n",
    "TIMEFORMAT 'auto';\n",
    "\n",
    "-- Validate\n",
    "SELECT COUNT(*) FROM stg.trips_raw;\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
