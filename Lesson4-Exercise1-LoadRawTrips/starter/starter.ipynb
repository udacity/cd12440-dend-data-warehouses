{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4 - Exercise 1: Load Staging Table with COPY Command\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this exercise, you will:\n",
    "\n",
    "1. Create your own S3 bucket for staging data\n",
    "2. Upload data files to S3\n",
    "3. Use the COPY command to load data into Redshift\n",
    "4. Understand COPY options for different file formats\n",
    "5. Validate successful data loads\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS credentials configured in `aws_config.py`\n",
    "- `van_transit_trips_postgres.csv` data file\n",
    "- Redshift Serverless workgroup available\n",
    "\n",
    "## Context\n",
    "\n",
    "In production Redshift workflows, data flows from ETL scripts to S3, then into Redshift via the COPY command. The COPY command reads files in parallel across all cluster nodes, making it far more efficient than row-by-row INSERTs.\n",
    "\n",
    "**Note on Authentication**: In production, you would use `IAM_ROLE` for secure access. In this workspace, we use temporary credentials with the `CREDENTIALS` parameter since no default IAM role is configured on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Imports\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"   - pandas version: {pd.__version__}\")\n",
    "print(f\"   - numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Load AWS Credentials\n",
    "import aws_config\n",
    "\n",
    "AWS_REGION = os.getenv('AWS_REGION')\n",
    "REDSHIFT_DATABASE = os.getenv('REDSHIFT_DATABASE')\n",
    "REDSHIFT_WORKGROUP = os.getenv('REDSHIFT_WORKGROUP')\n",
    "\n",
    "# Get credentials for COPY command\n",
    "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "AWS_SESSION_TOKEN = os.getenv('AWS_SESSION_TOKEN')\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"   - AWS Region: {AWS_REGION}\")\n",
    "print(f\"   - Redshift Database: {REDSHIFT_DATABASE}\")\n",
    "print(f\"   - Redshift Workgroup: {REDSHIFT_WORKGROUP}\")\n",
    "print(f\"   - Credentials: {'Loaded' if AWS_ACCESS_KEY_ID else 'Missing'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Helper Function: Execute Redshift Query\n",
    "def execute_redshift_query(sql, fetch_results=True):\n",
    "    \"\"\"Execute a SQL query on Redshift Serverless and optionally return results.\"\"\"\n",
    "    client = boto3.client('redshift-data', region_name=AWS_REGION)\n",
    "    \n",
    "    # Execute query\n",
    "    response = client.execute_statement(\n",
    "        WorkgroupName=REDSHIFT_WORKGROUP,\n",
    "        Database=REDSHIFT_DATABASE,\n",
    "        Sql=sql\n",
    "    )\n",
    "    query_id = response['Id']\n",
    "    \n",
    "    # Wait for completion\n",
    "    status = 'SUBMITTED'\n",
    "    while status in ['SUBMITTED', 'PICKED', 'STARTED']:\n",
    "        time.sleep(1)\n",
    "        status_response = client.describe_statement(Id=query_id)\n",
    "        status = status_response['Status']\n",
    "    \n",
    "    if status == 'FAILED':\n",
    "        error = status_response.get('Error', 'Unknown error')\n",
    "        raise Exception(f\"Query failed: {error}\")\n",
    "    \n",
    "    # Fetch results if requested\n",
    "    if fetch_results and status == 'FINISHED':\n",
    "        try:\n",
    "            result = client.get_statement_result(Id=query_id)\n",
    "            columns = [col['name'] for col in result['ColumnMetadata']]\n",
    "            rows = []\n",
    "            for record in result['Records']:\n",
    "                row = [list(field.values())[0] if field else None for field in record]\n",
    "                rows.append(row)\n",
    "            return pd.DataFrame(rows, columns=columns)\n",
    "        except client.exceptions.ResourceNotFoundException:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Create S3 Bucket for Staging\n",
    "\n",
    "We'll create a unique S3 bucket to store our staging data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Create S3 bucket\n",
    "s3 = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "# Generate unique bucket name\n",
    "unique_id = str(uuid.uuid4())[:8]\n",
    "BUCKET_NAME = f\"udacity-redshift-staging-{unique_id}\"\n",
    "\n",
    "try:\n",
    "    if AWS_REGION == 'us-east-1':\n",
    "        s3.create_bucket(Bucket=BUCKET_NAME)\n",
    "    else:\n",
    "        s3.create_bucket(\n",
    "            Bucket=BUCKET_NAME,\n",
    "            CreateBucketConfiguration={'LocationConstraint': AWS_REGION}\n",
    "        )\n",
    "    print(f\"SUCCESS: Created bucket '{BUCKET_NAME}'\")\n",
    "except ClientError as e:\n",
    "    if 'BucketAlreadyOwnedByYou' in str(e):\n",
    "        print(f\"Bucket already exists: {BUCKET_NAME}\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "print(f\"\\nS3 URI: s3://{BUCKET_NAME}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Review and Upload Transit Trip Data to S3\n",
    "\n",
    "We'll use the Vancouver Transit trips dataset which contains transit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Review the source data\n",
    "csv_path = \"data/van_transit_trips_postgres.csv\"\n",
    "\n",
    "trips_df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Dataset: {csv_path}\")\n",
    "print(f\"Shape: {trips_df.shape[0]:,} rows x {trips_df.shape[1]} columns\")\n",
    "print(f\"\\nColumns ({len(trips_df.columns)}):\")\n",
    "for i, col in enumerate(trips_df.columns, 1):\n",
    "    print(f\"   {i:2}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Preview the data\n",
    "print(\"Sample data (first 5 rows):\")\n",
    "trips_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Upload the CSV file to S3.\n",
    "\n",
    "Use `s3.put_object()` to upload the file to `staging/trips/van_transit_trips.csv` in your bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Upload CSV to S3\n",
    "s3_key = 'staging/trips/van_transit_trips.csv'\n",
    "\n",
    "# TODO: Upload the file to S3\n",
    "# Hint: Open the file with open(csv_path, 'rb') and use s3.put_object()\n",
    "\n",
    "\n",
    "# Verify upload\n",
    "response = s3.head_object(Bucket=BUCKET_NAME, Key=s3_key)\n",
    "file_size = response['ContentLength']\n",
    "\n",
    "print(f\"Uploaded: s3://{BUCKET_NAME}/{s3_key}\")\n",
    "print(f\"   - Records: {len(trips_df):,}\")\n",
    "print(f\"   - Size: {file_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Verify the upload\n",
    "response = s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix='staging/trips/')\n",
    "\n",
    "print(\"Files in staging/trips/:\")\n",
    "for obj in response.get('Contents', []):\n",
    "    print(f\"   - {obj['Key']} ({obj['Size']:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Create Staging Table in Redshift\n",
    "\n",
    "Before we can COPY data, we need a target table in Redshift. The table schema must match the CSV columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Create a staging table `public.stg_trips_raw` with 23 columns matching the CSV schema.\n",
    "\n",
    "| Column | Type |\n",
    "|--------|------|\n",
    "| trip_id | VARCHAR(32) |\n",
    "| rider_id | VARCHAR(32) |\n",
    "| route_id | VARCHAR(32) |\n",
    "| mode | VARCHAR(16) |\n",
    "| origin_station_id | VARCHAR(32) |\n",
    "| destination_station_id | VARCHAR(32) |\n",
    "| board_datetime | TIMESTAMP |\n",
    "| alight_datetime | TIMESTAMP |\n",
    "| country | VARCHAR(8) |\n",
    "| province | VARCHAR(8) |\n",
    "| fare_class | VARCHAR(16) |\n",
    "| payment_method | VARCHAR(32) |\n",
    "| transfers | INTEGER |\n",
    "| zones_charged | INTEGER |\n",
    "| distance_km | DECIMAL(10,2) |\n",
    "| base_fare_cad | DECIMAL(10,2) |\n",
    "| discount_rate | DECIMAL(5,3) |\n",
    "| discount_amount_cad | DECIMAL(10,2) |\n",
    "| yvr_addfare_cad | DECIMAL(10,2) |\n",
    "| total_fare_cad | DECIMAL(10,2) |\n",
    "| on_time_arrival | BOOLEAN |\n",
    "| service_disruption | BOOLEAN |\n",
    "| polyline_stations | VARCHAR(512) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Create staging table with correct schema\n",
    "create_table_sql = \"\"\"\n",
    "DROP TABLE IF EXISTS public.stg_trips_raw;\n",
    "\n",
    "CREATE TABLE public.stg_trips_raw (\n",
    "    -- TODO: Add all 23 columns with correct types\n",
    "    \n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "execute_redshift_query(create_table_sql, fetch_results=False)\n",
    "print(\"SUCCESS: Created table public.stg_trips_raw\")\n",
    "print(f\"   - Columns: 23 (matching CSV schema)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Load Data with COPY Command\n",
    "\n",
    "Now we'll use the COPY command to load data from S3 into Redshift.\n",
    "\n",
    "### COPY Command Components\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|----------|\n",
    "| `COPY table_name` | Target table for the load |\n",
    "| `FROM 's3://...'` | S3 path to data files |\n",
    "| `CREDENTIALS` | AWS credentials for S3 access (workspace) |\n",
    "| `IAM_ROLE` | IAM role ARN for S3 access (production) |\n",
    "| `FORMAT AS CSV` | File format |\n",
    "| `IGNOREHEADER 1` | Skip header row |\n",
    "\n",
    "### Production vs Workspace Authentication\n",
    "\n",
    "```sql\n",
    "-- PRODUCTION: Use IAM Role (recommended)\n",
    "COPY table FROM 's3://bucket/path/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789:role/RedshiftS3Role'\n",
    "FORMAT AS CSV;\n",
    "\n",
    "-- WORKSPACE: Use temporary credentials\n",
    "COPY table FROM 's3://bucket/path/'\n",
    "CREDENTIALS 'aws_access_key_id=...;aws_secret_access_key=...;token=...'\n",
    "FORMAT AS CSV;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Write the COPY command to load data from S3 into the staging table.\n",
    "\n",
    "Include these options:\n",
    "- `FORMAT AS CSV`\n",
    "- `IGNOREHEADER 1`\n",
    "- `TIMEFORMAT 'auto'`\n",
    "- `DATEFORMAT 'auto'`\n",
    "- `REGION '{AWS_REGION}'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Execute COPY command\n",
    "s3_path = f\"s3://{BUCKET_NAME}/staging/trips/\"\n",
    "\n",
    "# TODO: Write the COPY command\n",
    "copy_sql = f\"\"\"\n",
    "COPY public.stg_trips_raw\n",
    "FROM '{s3_path}'\n",
    "-- TODO: Add CREDENTIALS clause with AWS credentials\n",
    "-- TODO: Add FORMAT, IGNOREHEADER, TIMEFORMAT, DATEFORMAT, REGION options\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executing COPY command...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "execute_redshift_query(copy_sql, fetch_results=False)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nSUCCESS: COPY completed in {elapsed:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Validate the Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Verify the row count matches between CSV and database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Verify row count\n",
    "\n",
    "# TODO: Write a query to count rows in stg_trips_raw\n",
    "result = execute_redshift_query(\"-- TODO: Write COUNT query\")\n",
    "\n",
    "db_count = int(result['row_count'].iloc[0])\n",
    "csv_count = len(trips_df)\n",
    "\n",
    "print(\"Row Count Validation:\")\n",
    "print(f\"   - CSV file:  {csv_count:,} rows\")\n",
    "print(f\"   - Database:  {db_count:,} rows\")\n",
    "print(f\"   - Match: {'YES' if db_count == csv_count else 'NO - Check STL_LOAD_ERRORS'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Check for NULL values in key columns (trip_id, rider_id, board_datetime, fare_class, total_fare_cad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Check for NULL values in key columns\n",
    "null_check_sql = \"\"\"\n",
    "-- TODO: Write a query to check NULL counts for key columns\n",
    "-- Use SUM(CASE WHEN column IS NULL THEN 1 ELSE 0 END) pattern\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result = execute_redshift_query(null_check_sql)\n",
    "print(\"NULL Value Check:\")\n",
    "for col in result.columns:\n",
    "    print(f\"   - {col}: {result[col].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Sample data from staging table\n",
    "sample_sql = \"\"\"\n",
    "SELECT \n",
    "    trip_id, rider_id, route_id, mode, fare_class, \n",
    "    total_fare_cad, board_datetime\n",
    "FROM public.stg_trips_raw\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample Data from Staging Table:\")\n",
    "execute_redshift_query(sample_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Write a summary query showing total trips, unique riders, unique routes, fare classes, date range, and revenue metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Summary statistics\n",
    "summary_sql = \"\"\"\n",
    "-- TODO: Write summary query with:\n",
    "-- COUNT(*) as total_trips\n",
    "-- COUNT(DISTINCT rider_id) as unique_riders\n",
    "-- COUNT(DISTINCT route_id) as unique_routes\n",
    "-- COUNT(DISTINCT fare_class) as fare_classes\n",
    "-- MIN/MAX(board_datetime) for date range\n",
    "-- SUM/AVG(total_fare_cad) for revenue metrics\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Staging Table Summary:\")\n",
    "print(\"=\" * 60)\n",
    "result = execute_redshift_query(summary_sql)\n",
    "for col in result.columns:\n",
    "    print(f\"   {col}: {result[col].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: COPY Options Reference\n",
    "\n",
    "The COPY command supports many options for different data scenarios.\n",
    "\n",
    "### Common COPY Options\n",
    "\n",
    "| Option | Purpose | Example |\n",
    "|--------|---------|----------|\n",
    "| `IGNOREHEADER n` | Skip first n rows (header) | `IGNOREHEADER 1` |\n",
    "| `DELIMITER 'char'` | Column separator | `DELIMITER ','` |\n",
    "| `TIMEFORMAT` | Timestamp parsing | `TIMEFORMAT 'auto'` |\n",
    "| `DATEFORMAT` | Date parsing | `DATEFORMAT 'auto'` |\n",
    "| `BLANKSASNULL` | Empty strings become NULL | - |\n",
    "| `EMPTYASNULL` | Empty fields become NULL | - |\n",
    "| `MAXERROR n` | Allow up to n errors | `MAXERROR 100` |\n",
    "| `TRUNCATECOLUMNS` | Truncate data exceeding column width | - |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= COPY Command Reference Examples\n",
    "\n",
    "print(\"COPY Command Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example 1: Basic CSV load (Production with IAM Role)\n",
    "print(\"\"\"\n",
    "1. PRODUCTION: Basic CSV Load with IAM Role\n",
    "--------------------------------------------\n",
    "COPY public.stg_trips_raw\n",
    "FROM 's3://your-bucket/staging/trips/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
    "FORMAT AS CSV\n",
    "IGNOREHEADER 1;\n",
    "\"\"\")\n",
    "\n",
    "# Example 2: CSV with error handling\n",
    "print(\"\"\"\n",
    "2. Robust CSV Load with Error Handling\n",
    "--------------------------------------\n",
    "COPY public.stg_trips_raw\n",
    "FROM 's3://your-bucket/staging/trips/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
    "FORMAT AS CSV\n",
    "IGNOREHEADER 1\n",
    "DELIMITER ','\n",
    "TIMEFORMAT 'auto'\n",
    "DATEFORMAT 'auto'\n",
    "BLANKSASNULL\n",
    "EMPTYASNULL\n",
    "ACCEPTINVCHARS AS '?'\n",
    "MAXERROR 100\n",
    "TRUNCATECOLUMNS;\n",
    "\"\"\")\n",
    "\n",
    "# Example 3: Parquet format (recommended for production)\n",
    "print(\"\"\"\n",
    "3. Parquet Format (Recommended for Production)\n",
    "----------------------------------------------\n",
    "COPY public.stg_trips_raw\n",
    "FROM 's3://your-bucket/staging/trips/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
    "FORMAT AS PARQUET;\n",
    "\n",
    "Note: Parquet is 5-10x faster than CSV and doesn't need\n",
    "      IGNOREHEADER or DELIMITER options.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Check for Load Errors\n",
    "\n",
    "If your COPY command fails, you can check the error logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Check for COPY errors\n",
    "# Note: stl_load_errors requires superuser privileges\n",
    "# In this workspace, we may not have access to system tables\n",
    "\n",
    "error_sql = \"\"\"\n",
    "SELECT \n",
    "    starttime,\n",
    "    filename,\n",
    "    line_number,\n",
    "    colname,\n",
    "    err_reason\n",
    "FROM stl_load_errors\n",
    "ORDER BY starttime DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    errors = execute_redshift_query(error_sql)\n",
    "    if errors is not None and len(errors) > 0:\n",
    "        print(\"Recent COPY errors:\")\n",
    "        print(errors)\n",
    "    else:\n",
    "        print(\"No COPY errors found!\")\n",
    "except Exception as e:\n",
    "    if 'permission denied' in str(e).lower():\n",
    "        print(\"Note: Cannot access stl_load_errors (requires superuser privileges)\")\n",
    "        print(\"\\nIn production environments, you would check this table to debug COPY failures.\")\n",
    "        print(\"\\nAlternative: If COPY fails, the error message in the exception usually\")\n",
    "        print(\"contains enough information to diagnose the issue.\")\n",
    "    else:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup (Optional)\n",
    "\n",
    "Delete the S3 bucket when you're done to avoid storage charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= OPTIONAL: Delete S3 bucket\n",
    "# Uncomment the code below to delete the bucket after the exercise\n",
    "\n",
    "# print(\"Deleting S3 bucket and contents...\")\n",
    "# \n",
    "# # First delete all objects\n",
    "# response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "# for obj in response.get('Contents', []):\n",
    "#     s3.delete_object(Bucket=BUCKET_NAME, Key=obj['Key'])\n",
    "#     print(f\"   Deleted: {obj['Key']}\")\n",
    "# \n",
    "# # Then delete the bucket\n",
    "# s3.delete_bucket(Bucket=BUCKET_NAME)\n",
    "# print(f\"   Deleted bucket: {BUCKET_NAME}\")\n",
    "\n",
    "print(\"Cleanup skipped. Uncomment the code above to delete the S3 bucket.\")\n",
    "print(f\"\\nBucket to delete later: {BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **S3 Bucket Creation** - Created a unique bucket for staging data\n",
    "2. **Data Upload** - Uploaded CSV files to S3 using boto3\n",
    "3. **Table Creation** - Created a staging table matching the source schema\n",
    "4. **COPY Command** - Loaded data from S3 into Redshift using COPY\n",
    "5. **Validation** - Verified the data load with row counts and sample queries\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **COPY is faster** than INSERT for bulk loads (parallel across nodes)\n",
    "- **Schema must match** between source files and target table\n",
    "- **Check STL_LOAD_ERRORS** when loads fail or have fewer rows than expected\n",
    "- **Use IAM roles** in production instead of credentials strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production COPY Template\n",
    "\n",
    "```sql\n",
    "-- Idempotent staging load (PRODUCTION)\n",
    "TRUNCATE TABLE stg.trips_raw;\n",
    "\n",
    "COPY stg.trips_raw\n",
    "FROM 's3://your-bucket/staging/trips/'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3ReadRole'\n",
    "FORMAT AS CSV\n",
    "IGNOREHEADER 1\n",
    "TIMEFORMAT 'auto';\n",
    "\n",
    "-- Validate\n",
    "SELECT COUNT(*) FROM stg.trips_raw;\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
