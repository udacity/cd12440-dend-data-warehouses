{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-001",
   "metadata": {},
   "source": [
    "# Lesson 3: Exercise 1 - Extract and Transform Trips from PostgreSQL\n",
    "\n",
    "## Goal\n",
    "\n",
    "Extract trips data from PostgreSQL and transform it into a **staging format** ready for warehouse loading. This is the first step in our ETL pipeline.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You should have completed:\n",
    "- **Lesson 1, Exercise 1**: Connected to PostgreSQL and previewed the `raw_trips` table\n",
    "- **Lesson 2, Exercise 1**: Designed the `dw_dim_rider` table in Redshift\n",
    "\n",
    "## What You Will Build\n",
    "\n",
    "A Pandas-based ETL script that:\n",
    "\n",
    "1. Connects to PostgreSQL and extracts trips data\n",
    "2. Cleans and standardizes fields (whitespace, nulls, data types)\n",
    "3. Validates the transformation\n",
    "4. Outputs to staging format (CSV/Parquet)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports and Dependencies\n",
    "\n",
    "Run this cell first to import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"   - pandas version: {pd.__version__}\")\n",
    "print(f\"   - numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration\n",
    "\n",
    "**Important:** These credentials match the `populate-postgres.py` script from Lesson 1. Update only if your environment differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= PostgreSQL Configuration ==========\n",
    "# These match the populate-postgres.py script from Lesson 1.\n",
    "# Only change if your environment is different.\n",
    "\n",
    "PG_HOST = \"localhost\"      # Database host\n",
    "PG_PORT = \"5432\"           # Database port\n",
    "PG_DB = \"postgres\"         # Database name (populate script uses 'postgres')\n",
    "PG_USER = \"temp\"           # User from populate-postgres.py\n",
    "PG_PASSWORD = \"temp\"       # Password from populate-postgres.py\n",
    "\n",
    "# Build connection URI\n",
    "PG_URI = f\"postgresql://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_STAGING_CSV = \"/tmp/stg_trips_raw.csv\"\n",
    "OUTPUT_STAGING_PARQUET = \"/tmp/stg_trips_raw.parquet\"\n",
    "\n",
    "print(\"Configuration set!\")\n",
    "print(f\"   - PostgreSQL: {PG_HOST}:{PG_PORT}/{PG_DB}\")\n",
    "print(f\"   - User: {PG_USER}\")\n",
    "print(f\"   - Output CSV: {OUTPUT_STAGING_CSV}\")\n",
    "print(f\"   - Output Parquet: {OUTPUT_STAGING_PARQUET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "populate-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Populate the Database\n",
    "\n",
    "Run this cell to populate PostgreSQL with sample data (if not already done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "populate-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python populate-postgres.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colspec-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Column Specification\n",
    "\n",
    "Define the expected columns and their types for the staging table. This matches the structure we'll use in Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colspec-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Column specs for staging (name, kind)\n",
    "# kind: 's' = string, 'ts' = timestamp, 'i' = integer, 'f' = float, 'b' = boolean\n",
    "\n",
    "TRIPS_COLSPEC = [\n",
    "    ('trip_id', 's'),\n",
    "    ('rider_id', 's'),\n",
    "    ('route_id', 's'),\n",
    "    ('mode', 's'),\n",
    "    ('origin_station_id', 's'),\n",
    "    ('destination_station_id', 's'),\n",
    "    ('board_datetime', 'ts'),\n",
    "    ('alight_datetime', 'ts'),\n",
    "    ('country', 's'),\n",
    "    ('province', 's'),\n",
    "    ('fare_class', 's'),\n",
    "    ('payment_method', 's'),\n",
    "    ('transfers', 'i'),\n",
    "    ('zones_charged', 'i'),\n",
    "    ('distance_km', 'f'),\n",
    "    ('base_fare_cad', 'f'),\n",
    "    ('discount_rate', 'f'),\n",
    "    ('discount_amount_cad', 'f'),\n",
    "    ('yvr_addfare_cad', 'f'),\n",
    "    ('total_fare_cad', 'f'),\n",
    "    ('on_time_arrival', 'b'),\n",
    "    ('service_disruption', 'b'),\n",
    "    ('polyline_stations', 's'),\n",
    "]\n",
    "\n",
    "print(f\"Column spec defined: {len(TRIPS_COLSPEC)} columns\")\n",
    "print(\"\\nColumn breakdown:\")\n",
    "print(f\"   - String columns: {sum(1 for _, k in TRIPS_COLSPEC if k == 's')}\")\n",
    "print(f\"   - Timestamp columns: {sum(1 for _, k in TRIPS_COLSPEC if k == 'ts')}\")\n",
    "print(f\"   - Integer columns: {sum(1 for _, k in TRIPS_COLSPEC if k == 'i')}\")\n",
    "print(f\"   - Float columns: {sum(1 for _, k in TRIPS_COLSPEC if k == 'f')}\")\n",
    "print(f\"   - Boolean columns: {sum(1 for _, k in TRIPS_COLSPEC if k == 'b')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpers-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Helper Functions\n",
    "\n",
    "These functions are adapted from the project solution for cleaning and transforming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpers-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Helper Functions (from project solution)\n",
    "\n",
    "def trim_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize text fields (strip whitespace) and handle NaN values properly.\n",
    "    \n",
    "    This is the same helper used in the final project for cleaning extracted data.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame with potentially messy string fields\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned DataFrame with stripped strings and proper NaN handling\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Avoid modifying the original DataFrame\n",
    "    for c in df.select_dtypes(include=['object']).columns:\n",
    "        # Convert to string and strip whitespace\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "        # Replace 'nan', 'None', and empty strings with actual NaN\n",
    "        df[c] = df[c].replace({'nan': np.nan, 'None': np.nan, 'NaN': np.nan, '': np.nan})\n",
    "    return df\n",
    "\n",
    "\n",
    "def validate_columns(df: pd.DataFrame, colspec: List[Tuple[str, str]]) -> dict:\n",
    "    \"\"\"\n",
    "    Validate that DataFrame columns match the expected spec.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to validate\n",
    "        colspec: List of (column_name, type_code) tuples\n",
    "    \n",
    "    Returns:\n",
    "        Dict with validation results\n",
    "    \"\"\"\n",
    "    expected_cols = {c for c, _ in colspec}\n",
    "    actual_cols = set(df.columns)\n",
    "    \n",
    "    return {\n",
    "        'missing': expected_cols - actual_cols,\n",
    "        'extra': actual_cols - expected_cols,\n",
    "        'matched': expected_cols & actual_cols,\n",
    "        'valid': expected_cols == actual_cols\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Helper functions defined: trim_df(), validate_columns()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Connect to PostgreSQL\n",
    "\n",
    "Establish a connection to the PostgreSQL database using SQLAlchemy (same pattern as Lesson 1, Exercise 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= STEP 1: Connect to PostgreSQL\n",
    "print(\"Step 1: Connecting to PostgreSQL...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "engine = create_engine(PG_URI)\n",
    "\n",
    "# Test the connection\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT 1\"))\n",
    "    print(\"Successfully connected to PostgreSQL!\")\n",
    "    \n",
    "    # Get row count\n",
    "    count_result = conn.execute(text(\"SELECT COUNT(*) FROM raw_trips\"))\n",
    "    total_rows = count_result.scalar()\n",
    "    print(f\"Total rows in raw_trips: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Extract Data from PostgreSQL\n",
    "\n",
    "Pull all trips data from the `raw_trips` table. In production ETL, you might add filters for incremental loads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract-todo",
   "metadata": {},
   "source": [
    "**TODO**: Write a SQL query to extract all columns from `raw_trips`. The columns should match the TRIPS_COLSPEC defined above:\n",
    "\n",
    "- trip_id, rider_id, route_id, mode\n",
    "- origin_station_id, destination_station_id\n",
    "- board_datetime, alight_datetime\n",
    "- country, province, fare_class, payment_method\n",
    "- transfers, zones_charged, distance_km\n",
    "- base_fare_cad, discount_rate, discount_amount_cad, yvr_addfare_cad, total_fare_cad\n",
    "- on_time_arrival, service_disruption, polyline_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= STEP 2: Extract from PostgreSQL\n",
    "print(\"Step 2: Extracting trips data from PostgreSQL...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# TODO: Write your SQL query to extract all trips columns\n",
    "SQL_EXTRACT = \"\"\"\n",
    "-- TODO: Write your SELECT statement here\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    trips_raw = pd.read_sql(text(SQL_EXTRACT), conn)\n",
    "\n",
    "print(f\"Extracted {len(trips_raw):,} rows\")\n",
    "print(f\"Columns: {len(trips_raw.columns)}\")\n",
    "print(f\"\\nSample data (first 3 rows):\")\n",
    "display(trips_raw.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transform-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Transform and Clean Data\n",
    "\n",
    "Apply transformations to prepare the data for staging:\n",
    "- Strip whitespace from string fields\n",
    "- Standardize NULL representations\n",
    "- Parse timestamps\n",
    "- Ensure proper data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transform-todo",
   "metadata": {},
   "source": [
    "**TODO**: Complete the transformation logic:\n",
    "\n",
    "1. Apply `trim_df()` to clean string fields\n",
    "2. Parse timestamp columns using `pd.to_datetime()` with `errors='coerce'`\n",
    "3. Convert boolean columns (handle string representations like 'true', 'false')\n",
    "4. Ensure numeric columns have proper types using `pd.to_numeric()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transform-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= STEP 3: Transform and clean\n",
    "print(\"Step 3: Transforming and cleaning data...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# TODO: Apply string cleaning using trim_df()\n",
    "trips_clean = trips_raw  # Replace with: trim_df(trips_raw)\n",
    "\n",
    "# TODO: Parse timestamp columns\n",
    "timestamp_cols = [c for c, k in TRIPS_COLSPEC if k == 'ts']\n",
    "for col in timestamp_cols:\n",
    "    if col in trips_clean.columns:\n",
    "        # TODO: Convert column to datetime\n",
    "        pass\n",
    "\n",
    "# TODO: Ensure boolean columns are proper booleans\n",
    "boolean_cols = [c for c, k in TRIPS_COLSPEC if k == 'b']\n",
    "for col in boolean_cols:\n",
    "    if col in trips_clean.columns:\n",
    "        # TODO: Handle string boolean representations\n",
    "        pass\n",
    "\n",
    "# TODO: Ensure numeric columns are proper types\n",
    "float_cols = [c for c, k in TRIPS_COLSPEC if k == 'f']\n",
    "for col in float_cols:\n",
    "    if col in trips_clean.columns:\n",
    "        # TODO: Convert to numeric\n",
    "        pass\n",
    "\n",
    "int_cols = [c for c, k in TRIPS_COLSPEC if k == 'i']\n",
    "for col in int_cols:\n",
    "    if col in trips_clean.columns:\n",
    "        # TODO: Convert to integer (use 'Int64' for nullable integers)\n",
    "        pass\n",
    "\n",
    "print(f\"Transformation complete!\")\n",
    "print(f\"   - Rows: {len(trips_clean):,}\")\n",
    "print(f\"   - Columns: {len(trips_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validate-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Validate the Transformation\n",
    "\n",
    "Before outputting, verify that:\n",
    "- All expected columns are present\n",
    "- Row counts match (no data loss)\n",
    "- Key fields have no unexpected nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= STEP 4: Validate\n",
    "print(\"Step 4: Validating transformation...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check column alignment\n",
    "validation = validate_columns(trips_clean, TRIPS_COLSPEC)\n",
    "\n",
    "if validation['valid']:\n",
    "    print(\"Column validation: PASSED\")\n",
    "else:\n",
    "    print(\"Column validation: ISSUES FOUND\")\n",
    "    if validation['missing']:\n",
    "        print(f\"   Missing columns: {validation['missing']}\")\n",
    "    if validation['extra']:\n",
    "        print(f\"   Extra columns: {validation['extra']}\")\n",
    "\n",
    "# Check row counts\n",
    "print(f\"\\nRow count check:\")\n",
    "print(f\"   Source rows: {len(trips_raw):,}\")\n",
    "print(f\"   Output rows: {len(trips_clean):,}\")\n",
    "print(f\"   Match: {'YES' if len(trips_raw) == len(trips_clean) else 'NO - DATA LOSS!'}\")\n",
    "\n",
    "# Check for nulls in key fields\n",
    "key_fields = ['trip_id', 'rider_id', 'route_id']\n",
    "print(f\"\\nNull check for key fields:\")\n",
    "for field in key_fields:\n",
    "    null_count = trips_clean[field].isna().sum()\n",
    "    print(f\"   {field}: {null_count} nulls ({null_count/len(trips_clean)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Output to Staging Format\n",
    "\n",
    "Save the transformed data in formats suitable for warehouse loading:\n",
    "- **CSV**: Human-readable, compatible with Redshift COPY\n",
    "- **Parquet**: Compressed, columnar format for efficient loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output-todo",
   "metadata": {},
   "source": [
    "**TODO**: Write the staging DataFrame to CSV:\n",
    "\n",
    "1. Select only the columns defined in TRIPS_COLSPEC (in order)\n",
    "2. Save to CSV using `to_csv()` with `index=False`\n",
    "3. Report the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "output-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= STEP 5: Output to staging format\n",
    "print(\"Step 5: Outputting to staging format...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Select only the columns defined in the spec (in order)\n",
    "output_cols = [c for c, _ in TRIPS_COLSPEC]\n",
    "trips_staging = trips_clean[output_cols]\n",
    "\n",
    "# TODO: Output to CSV\n",
    "# trips_staging.to_csv(OUTPUT_STAGING_CSV, index=False)\n",
    "\n",
    "# TODO: Report file size\n",
    "# file_size = os.path.getsize(OUTPUT_STAGING_CSV) / 1024\n",
    "# print(f\"CSV saved: {OUTPUT_STAGING_CSV}\")\n",
    "# print(f\"   Size: {file_size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= STEP 6: Verify output\n",
    "print(\"Step 6: Verifying output...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Read back the CSV\n",
    "trips_verify = pd.read_csv(OUTPUT_STAGING_CSV)\n",
    "\n",
    "print(f\"Read back {len(trips_verify):,} rows from CSV\")\n",
    "print(f\"Columns match: {list(trips_verify.columns) == output_cols}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(f\"\\nFirst 3 rows of staged data:\")\n",
    "display(trips_verify.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Clean Up\n",
    "\n",
    "Close the database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= STEP 7: Clean up\n",
    "engine.dispose()\n",
    "print(\"PostgreSQL connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Generate a final summary report of the ETL job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Final Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"ETL JOB SUMMARY: Trips to Staging\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Source:           PostgreSQL {PG_HOST}:{PG_PORT}/{PG_DB}\n",
    "Table:            raw_trips\n",
    "Output CSV:       {OUTPUT_STAGING_CSV}\n",
    "\n",
    "Records:\n",
    "  - Extracted:    {len(trips_raw):,}\n",
    "  - Transformed:  {len(trips_clean):,}\n",
    "  - Staged:       {len(trips_staging):,}\n",
    "\n",
    "Data Quality:\n",
    "  - Columns:      {len(trips_staging.columns)} (all expected)\n",
    "  - Null trip_id: {trips_staging['trip_id'].isna().sum()}\n",
    "\n",
    "Status:           SUCCESS\n",
    "Completed:        {datetime.now().isoformat()}\n",
    "\"\"\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
