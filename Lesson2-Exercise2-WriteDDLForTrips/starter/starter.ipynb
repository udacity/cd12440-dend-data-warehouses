{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-001",
   "metadata": {},
   "source": [
    "# Lesson 2: Exercise 2 - Design the `dw_fact_trips` Table\n",
    "\n",
    "## Goal\n",
    "\n",
    "Define the core trips fact table (1 row per `trip_id`) with correct **grain**, **foreign-key columns** (SKs to dimensions), **measures**, and **Redshift physical design** (distribution/sort/encodings).\n",
    "\n",
    "## What You Will Build\n",
    "\n",
    "Create `dw_fact_trips` with:\n",
    "\n",
    "- **Grain**: 1 row per `trip_id`\n",
    "- **Foreign keys**: `rider_sk`, `route_sk`, `mode_sk`, `origin_station_sk`, `destination_station_sk`\n",
    "- **Date keys**: Integer `YYYYMMDD` format (`board_date_key`, `alight_date_key`)\n",
    "- **Measures**: fares, distance, transfers, zones\n",
    "- **Flags**: `on_time_arrival`, `service_disruption`\n",
    "- **Distribution**: collocate rider-centric queries\n",
    "- **Sort**: time-based pruning for range filters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports and Dependencies\n",
    "\n",
    "Run this cell first to import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Imports\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"   - pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration\n",
    "\n",
    "Configure your Redshift connection. The same pattern is used in the final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= CONFIG (edit for your environment)\n",
    "# Set your AWS credentials in the aws_config.py file\n",
    "from aws_config import *  # This sets all AWS env vars\n",
    "\n",
    "# ---- Read configuration from environment\n",
    "AWS_ACCESS_KEY_ID           = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY       = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_SESSION_TOKEN           = os.getenv(\"AWS_SESSION_TOKEN\")\n",
    "AWS_REGION                  = os.getenv(\"AWS_REGION\")\n",
    "REDSHIFT_DATABASE           = os.getenv(\"REDSHIFT_DATABASE\")\n",
    "REDSHIFT_WORKGROUP          = os.getenv(\"REDSHIFT_WORKGROUP\")\n",
    "REDSHIFT_SECRET_ARN         = os.getenv(\"REDSHIFT_SECRET_ARN\")            # Optional\n",
    "REDSHIFT_CLUSTER_IDENTIFIER = os.getenv(\"REDSHIFT_CLUSTER_IDENTIFIER\")    # For provisioned\n",
    "REDSHIFT_DB_USER            = os.getenv(\"REDSHIFT_DB_USER\")               # For provisioned\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"   - AWS Region: {AWS_REGION}\")\n",
    "print(f\"   - Redshift: {REDSHIFT_DATABASE} (workgroup: {REDSHIFT_WORKGROUP})\")\n",
    "print()\n",
    "if AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY:\n",
    "    print(f\"   AWS credentials found (Key ID: {AWS_ACCESS_KEY_ID[:8]}...)\")\n",
    "    if AWS_SESSION_TOKEN:\n",
    "        print(f\"   AWS session token found (temporary credentials)\")\n",
    "else:\n",
    "    print(\"   WARNING: AWS credentials NOT FOUND!\")\n",
    "    print(\"      Redshift operations will fail with 'NoCredentialsError'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "redshift-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Redshift Functions\n",
    "\n",
    "These helper functions match the patterns used in the final project. Learning them here will prepare you for the capstone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "redshift-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Redshift Functions\n",
    "\n",
    "session_boto = boto3.Session(region_name=AWS_REGION)\n",
    "rsd = session_boto.client(\"redshift-data\", region_name=AWS_REGION)\n",
    "\n",
    "\n",
    "def _rs_kwargs() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Shared Redshift Data API connection args.\n",
    "    \n",
    "    Supports both:\n",
    "    - Serverless: uses WorkgroupName (and optionally SecretArn)\n",
    "    - Provisioned: uses ClusterIdentifier and DbUser\n",
    "    \"\"\"\n",
    "    base = dict(Database=REDSHIFT_DATABASE)\n",
    "    if REDSHIFT_WORKGROUP:\n",
    "        base[\"WorkgroupName\"] = REDSHIFT_WORKGROUP\n",
    "        if REDSHIFT_SECRET_ARN:\n",
    "            base[\"SecretArn\"] = REDSHIFT_SECRET_ARN\n",
    "    elif REDSHIFT_CLUSTER_IDENTIFIER and REDSHIFT_DB_USER:\n",
    "        base[\"ClusterIdentifier\"] = REDSHIFT_CLUSTER_IDENTIFIER\n",
    "        base[\"DbUser\"] = REDSHIFT_DB_USER\n",
    "    else:\n",
    "        raise RuntimeError(\"Configure Redshift serverless OR provisioned for Data API.\")\n",
    "    return base\n",
    "\n",
    "\n",
    "def rs_exec(sql: str, params: List[Dict[str, Any]] = None, return_results=False, timeout_s=900):\n",
    "    \"\"\"\n",
    "    Execute SQL on Redshift via the Data API.\n",
    "    \n",
    "    Args:\n",
    "        sql: SQL statement to execute\n",
    "        params: Optional list of parameter dicts for parameterized queries\n",
    "        return_results: If True, fetch and return query results\n",
    "        timeout_s: Maximum seconds to wait for query completion (default 15 min)\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts if return_results=True or query is SELECT, else None\n",
    "    \"\"\"\n",
    "    sql = sql.strip()\n",
    "    if not sql:\n",
    "        return None\n",
    "    \n",
    "    # Build request kwargs\n",
    "    kwargs = _rs_kwargs()\n",
    "    kwargs[\"Sql\"] = sql\n",
    "    if params:\n",
    "        kwargs[\"Parameters\"] = params\n",
    "    \n",
    "    # Execute statement\n",
    "    sid = rsd.execute_statement(**kwargs)[\"Id\"]\n",
    "    \n",
    "    # Poll for completion\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        d = rsd.describe_statement(Id=sid)\n",
    "        if d[\"Status\"] in (\"FINISHED\", \"FAILED\", \"ABORTED\"):\n",
    "            break\n",
    "        if time.time() - start > timeout_s:\n",
    "            raise TimeoutError(\"Redshift statement timeout\")\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Check for errors\n",
    "    if d[\"Status\"] != \"FINISHED\":\n",
    "        raise RuntimeError(f\"Redshift SQL failed: {d.get('Error')}\\n---\\n{sql}\")\n",
    "    \n",
    "    # Return results for SELECT queries or when explicitly requested\n",
    "    if return_results or sql.lower().startswith(\"select\"):\n",
    "        out, next_token = [], None\n",
    "        while True:\n",
    "            args = dict(Id=sid)\n",
    "            if next_token:\n",
    "                args[\"NextToken\"] = next_token\n",
    "            r = rsd.get_statement_result(**args)\n",
    "            cols = [c[\"name\"] for c in r[\"ColumnMetadata\"]]\n",
    "            for rec in r[\"Records\"]:\n",
    "                row = []\n",
    "                for cell in rec:\n",
    "                    row.append(next(iter(cell.values())))\n",
    "                out.append(dict(zip(cols, row)))\n",
    "            next_token = r.get(\"NextToken\")\n",
    "            if not next_token:\n",
    "                break\n",
    "        return out\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"Redshift functions defined: _rs_kwargs(), rs_exec()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddl-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Design the dw_fact_trips Table DDL\n",
    "\n",
    "The fact table captures **one row per trip** with:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|----------|\n",
    "| **Surrogate key** (`trip_sk`) | Warehouse-generated ID |\n",
    "| **Natural key** (`trip_id`) | Original ID from source system |\n",
    "| **Dimension FKs** | Surrogate keys linking to dimension tables |\n",
    "| **Date keys** | Integer `YYYYMMDD` format for efficient joins/filters |\n",
    "| **Measures** | Numeric facts (fares, distance, transfers) |\n",
    "| **Flags** | Boolean indicators for operational metrics |\n",
    "| **DISTKEY/SORTKEY** | Physical design for query performance |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddl-todo",
   "metadata": {},
   "source": [
    "**TODO**: Write the DDL statement to create `public.dw_fact_trips`. Your DDL should include:\n",
    "\n",
    "1. `DROP TABLE IF EXISTS` to allow re-running\n",
    "2. `CREATE TABLE` with:\n",
    "   - **Surrogate key**: `trip_sk` - BIGINT IDENTITY(1,1)\n",
    "   - **Natural key**: `trip_id` - VARCHAR(32) ENCODE zstd\n",
    "   - **Dimension FKs** (all BIGINT ENCODE zstd):\n",
    "     - `rider_sk`, `route_sk`, `mode_sk`\n",
    "     - `origin_station_sk`, `destination_station_sk`\n",
    "   - **Date keys** (INTEGER ENCODE zstd):\n",
    "     - `board_date_key`, `alight_date_key`\n",
    "   - **Trip measures**:\n",
    "     - `transfers` - INTEGER\n",
    "     - `zones_charged` - INTEGER\n",
    "     - `distance_km` - DECIMAL(10,2)\n",
    "   - **Fare measures** (all DECIMAL with ENCODE zstd):\n",
    "     - `base_fare_cad` - DECIMAL(12,2)\n",
    "     - `discount_rate` - DECIMAL(5,3)\n",
    "     - `discount_amount_cad` - DECIMAL(12,2)\n",
    "     - `yvr_addfare_cad` - DECIMAL(12,2)\n",
    "     - `total_fare_cad` - DECIMAL(12,2)\n",
    "   - **Additional FKs**: `payment_method_sk`, `fare_class_sk` - BIGINT\n",
    "   - **Flags**: `on_time_arrival`, `service_disruption` - BOOLEAN\n",
    "3. `DISTKEY (rider_sk)` - collocate for rider-centric analytics\n",
    "4. `SORTKEY (board_date_key)` - enable time-range query optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddl-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "DDL_FACT_TRIPS = \"\"\"\n",
    "-- TODO: Write your DDL here\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"DDL for dw_fact_trips:\")\n",
    "print(\"=\" * 60)\n",
    "print(DDL_FACT_TRIPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execute-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Execute the DDL\n",
    "\n",
    "Create the `dw_fact_trips` table in Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_exec(DDL_FACT_TRIPS)\n",
    "print(\"Table public.dw_fact_trips created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validate-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Validate the Table Structure\n",
    "\n",
    "Verify the table was created with all expected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sql = \"\"\"\n",
    "SELECT \n",
    "    column_name,\n",
    "    data_type,\n",
    "    character_maximum_length,\n",
    "    numeric_precision,\n",
    "    numeric_scale,\n",
    "    is_nullable\n",
    "FROM information_schema.columns\n",
    "WHERE table_schema = 'public'\n",
    "  AND table_name = 'dw_fact_trips'\n",
    "ORDER BY ordinal_position;\n",
    "\"\"\"\n",
    "\n",
    "columns = rs_exec(validation_sql, return_results=True)\n",
    "\n",
    "print(\"Table Structure for dw_fact_trips:\")\n",
    "print(\"-\" * 60)\n",
    "if columns:\n",
    "    df = pd.DataFrame(columns)\n",
    "    display(df)\n",
    "    print(f\"\\nTotal columns: {len(columns)}\")\n",
    "else:\n",
    "    print(\"No columns found. Check if table was created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "keys-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Check Distribution and Sort Keys\n",
    "\n",
    "Verify that the DISTKEY and SORTKEY were applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "keys-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_sql = \"\"\"\n",
    "SELECT \n",
    "    \"column\",\n",
    "    type,\n",
    "    encoding,\n",
    "    distkey,\n",
    "    sortkey\n",
    "FROM pg_table_def\n",
    "WHERE schemaname = 'public'\n",
    "  AND tablename = 'dw_fact_trips'\n",
    "ORDER BY \n",
    "    CASE WHEN distkey THEN 0 ELSE 1 END,\n",
    "    sortkey DESC,\n",
    "    \"column\";\n",
    "\"\"\"\n",
    "\n",
    "properties = rs_exec(properties_sql, return_results=True)\n",
    "\n",
    "print(\"Distribution and Sort Key Configuration:\")\n",
    "print(\"-\" * 60)\n",
    "if properties:\n",
    "    df = pd.DataFrame(properties)\n",
    "    display(df)\n",
    "    \n",
    "    # Summarize key columns\n",
    "    distkey_col = [p[\"column\"] for p in properties if p.get(\"distkey\")]\n",
    "    sortkey_col = [p[\"column\"] for p in properties if p.get(\"sortkey\") and p[\"sortkey\"] > 0]\n",
    "    \n",
    "    print(f\"\\nDISTKEY column(s): {distkey_col}\")\n",
    "    print(f\"SORTKEY column(s): {sortkey_col}\")\n",
    "else:\n",
    "    print(\"Could not retrieve table properties.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rationale-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Design Rationale\n",
    "\n",
    "### Why This Design?\n",
    "\n",
    "| Design Choice | Rationale |\n",
    "|---------------|------------|\n",
    "| **Grain: 1 row per trip** | Each row represents one rider's journey from origin to destination. This matches the business event we want to measure. |\n",
    "| **DISTKEY on `rider_sk`** | Collocates trip facts with `dw_dim_rider` rows. Enables fast joins for rider cohort analysis, lifetime value, and behavior patterns. |\n",
    "| **SORTKEY on `board_date_key`** | Enables efficient time-window queries (e.g., \"trips last month\"). Also speeds up materialized view refreshes that filter by date. |\n",
    "| **Integer date keys** | `YYYYMMDD` format (e.g., 20240115) is compact, sortable, and joins efficiently with `dw_dim_date`. |\n",
    "| **Separate fare columns** | Breaking down `base_fare`, `discount`, `yvr_addfare`, and `total_fare` enables detailed revenue analysis. |\n",
    "| **Boolean flags** | `on_time_arrival` and `service_disruption` support operational KPIs without additional joins. |\n",
    "\n",
    "### Grain\n",
    "\n",
    "**1 row = 1 rider trip** (from board to alight)\n",
    "\n",
    "### Dimensional Relationships\n",
    "\n",
    "```\n",
    "dw_fact_trips connects to:\n",
    "  - dw_dim_rider (rider_sk)\n",
    "  - dw_dim_route (route_sk)\n",
    "  - dw_dim_mode (mode_sk)\n",
    "  - dw_dim_station (origin_station_sk, destination_station_sk)\n",
    "  - dw_dim_date (board_date_key, alight_date_key)\n",
    "  - dw_dim_payment_method (payment_method_sk)\n",
    "  - dw_dim_fare_class (fare_class_sk)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
